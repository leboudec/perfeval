\begin{minipage}[b]{0.5\textwidth}
Perhaps the most difficult part in performance
evaluation is where to start. In this chapter we
propose a methodology, i.e. a set of
recommendations valid for any performance
evaluation study. We stress the importance of
factors, in particular hidden factors, and the
need to use the scientific method. We also
discuss a few frequent performance patterns, as a
means to quickly focus on important issues.
~\\~\\~\\~\\
\end{minipage}
 ~
\begin{minipage}[b]{0.5\textwidth}
\hspace{1cm} ~~~~
\insfig{scouacMesureOrdiSigne}{0.9} ~\\
%~\\
%~\\
%~\\
\end{minipage}

\minitoc
\section{What is Performance
Evaluation~?}\label{sec-cose} In the context of this book,
performance evaluation is about quantifying the service
delivered by a computer or communication system. For example,
we might be interested in: comparing the power consumption of
several server farm configurations; knowing the response time
experienced by a customer performing a reservation over the
Internet; comparing compilers for a multiprocessor machine.

In all cases it is important to carefully define the \imp{load} and
the \imp{metric}, and to be aware of the performance evaluation
\imp{goals}.

\subsection{Load}

An important feature of computer or communication systems is that
their performance depends dramatically on the \nt{workload} (or
simply \nt{load}) they are subjected to. The load characterizes the
quantity and the nature of requests submitted to the system.
Consider for example the problem of quantifying the performance of a
web server. We could characterize the load by a simple concept such
as the number of requests per second. This is called the
\nt{intensity of the workload}. In general, the performance
deteriorates when the intensity increases, but often the
deterioration is sudden; this is due to the non-linearity of queuing
systems -- an example of \nt{performance pattern} that is discussed
in \sref{sec-pat} and \cref{ch-queuing}.

The performance of a system depends not only on the intensity of the
workload, but also its nature; for example, on a web server, all
requests are not equivalent: some web server softwares might perform
well with \emph{get} requests for frequently used objects, and less
well with requests that require database access; for other web
servers, things might be different. This is addressed by using
standardized mixes of web server requests. They are generated by a
\nt{benchmark}, defined as a load generation process that intends to
mimic a typical user behaviour. In \cref{ch-modfit} we study how
such a benchmark can be constructed.

\subsection{Metric}
\label{metric} A performance \nt{metric} is a measurable
quantity that precisely captures what we want to measure -- it
can take many forms. There is no general definition of a
performance metric: it is system dependent, and its definition
requires understanding the system and its users well. We will
often mention examples where the metric is throughput (number
of tasks completed per time unit), power consumption (integral
of the electrical energy consumed by the system, per time
unit), or response time (time elapsed between a start and an
end events). For each performance metric, we may be interested
in average, 95-percentile, worst-case, etc, as explained in
\cref{ch-conf}.

\begin{ex}{Windows versus Linux}
Chen and co-authors compare Windows versus Linux in
\cite{chen1995mpp}. They use as metric: number of CPU cycles, number
of instructions, number of data read/write operations required by a
typical job. The load was generated by various benchmarks:
``syscall" generates elementary operations (system calls); ``memory
read" generates references to an array;  an application benchmark
runs a popular application.
\end{ex}

It is also important to be aware of the experimental conditions
under which the metric is measured, as illustrated by the coming
example:
\begin{ex}{Power Consumption}
The electrical power consumed by a computer or telecom equipment
depends on how efficiently the equipment can take advantage of low
activity periods to save energy. One operator proposes the following
metric as a measure of power consumption \cite{verizon2008eer}:
 \ben
  P_{\midwor{Total}} = 0.35 P_{\midwor{max}}+ 0.4 P_{50} + 0.25
  P_{\midwor{sleep}}
 \een
 where $P_{\midwor{Total}}$ is the power consumption when the
 equipment is running at full load, $P_{50}$ when it is submitted to
 a load equal to 50\% of its
 capacity and $P_{\midwor{sleep}}$ when
 it is idle.
 The example uses weights ($0.35$, $0.4$ and $0.25$); they reflect our assumption
 about the proportion of time that a given load condition typically occurs
 (for example,the full load condition is assumed to occur during 35\% of the
 time).
\end{ex}
In this example, \nt{utilization} is a parameter
of the operating conditions. The utilization of a
resource
 is defined as the proportion of time that the resource is busy.

The example also illustrates that it may be
important to define which \imp{sampling method}
is used, i.e. when the measurements are taken.
This is an integral part of the definition of the
metric; we discuss this point in more detail in
\cref{ch-palm}.

A metric may be simple, i.e. expressed by a
single number (e.g. power consumption), or
\imp{multidimensional}, i.e. expressed by a
vector of several numbers (e.g. power
consumption, response time and throughput). When
comparing two vectors of multidimensional metric
values, one should compare the corresponding
components (e.g. power consumption of A versus
power consumption of B, response time of A versus
response time of B, etc). As a result, it may
happen that none of the two vectors is better
than the other. We say that comparison of vectors
is a \nt{partial order}, as opposed to comparison
of numbers which is a \nt{complete order}. It is
however useful to determine whether a vector is
\nt{non-dominated}, i.e. there is no other vector
(in the set of available results) which is
better. In a finite set of performance results
expressed with a multidimensional metric, there
are usually more than one non-dominated results.
When comparing several configurations, the
non-dominated ones are the only ones of interest.

\begin{ex}{Multi-dimensional Metric and Kiviat Diagram}
We measure the performance of a web server
submitted to the load of a standard workbench. We
compare 5 different configurations, and obtain
the results below.
%\begin{table}
  \begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    \input{multidim}
  \end{tabular}
  \end{center}
  %\mycaption{Data for Kiviat Plot}\label{tab-multidim}
%\end{table}
We see for example that configuration A is better
than B but is not better than D. There are two
non dominated configurations: A and D. A is
better on power consumption, D is better on
throughput and response time.

The numerical values can be visualized on a
\nt{Kiviat Diagram} (also called Radar graph or
\nt{Spider Plot} as on
\fref{fig-multidim}.\label{ex-kiviat}
\end{ex}
\begin{figure}
  \insfig{multidim}{0.7}
  \mycaption{Visualisation of the data in \exref{ex-kiviat} by means of
  a Kiviat Diagram. Configurations A and D are non-dominated.}\label{fig-multidim}
\end{figure}

\subsection{The Different Goals of Performance Evaluation}

The goal of a performance evaluation may either be a \nt{comparison}
of design alternatives, i.e. quantify the improvement brought by a
design option, or \nt{system dimensioning}, i.e. determine the size
of all system components for a given planned utilization. Comparison
of designs requires a well-defined load model; however, the exact
value of its intensity does not have to be identified. In contrast,
system dimensioning requires a detailed estimation of the load
intensity. Like any prediction exercise, this is very hazardous. For
any performance evaluation, it is important to know whether the
results depend on a workload prediction or not. Simple forecasting
techniques are discussed \cref{ch-forecast}.

\begin{ex}{Different Goals}
\mq{metho-q2}{Say which is the nature of goal for each of the
following performance evaluations statements:}{(A1), (A3) are
comparisons of design options; (A2) is dimensioning}
\begin{itemize}
  \item[(A1)] PC configuration 1 is 25\% faster than PC configuration 2
  when running Photoshop.
  \item[(A2)] For your video on demand application, the number of required servers is 35,
   and the number of disk units is $68$.
  \item[(A3)] Using the new version of \texttt{sendfile()} increases the server throughput by 51\%
\end{itemize}
\end{ex}

The benefit of a performance evaluation study has to be weighted
against its cost and the cost of the system. In practice, detailed
performance evaluations are done by product development units
(system design). During system operation, it is not economical
(except for huge systems such as public communication networks) to
do so. Instead, manufacturers provide \nt{engineering rules}, which
capture the relation between load intensity and performance. Example
(A2) above is probably best replaced by an engineering rule such as:
\begin{ex}{Engineering Rule}
\begin{itemize}
  \item[(E2)] For your video on demand application, the number of required servers is given by $N_1=\lceil
  \frac{R}{59.3}+\frac{B}{3.6}\rceil$ and the number of disk units by $N_2=\lceil
  \frac{R}{19.0}+\frac{B}{2.4}\rceil$, where $R$ [resp. $B$] is the number of residential [resp.
  business] customers ($\lceil x\rceil$ is the ceiling of $x$, i.e. the smallest integer $\geq x$).
\end{itemize}
\end{ex}
 In this book, we study the techniques of performance evaluation that apply
 to  all these cases. However, how to implement a high performance system
(for example: how to efficiently code a real time application in
Linux) or how to design bug-free systems are \emph{outside} the
scope.

%
%\begin{ex}{}
%Consider the following performance evaluation results:
%\begin{itemize}
%  \item[(A1)] PC configuration 1 is 25\% faster than PC configuration 2 when running Excel
%  \item[(A2)] For your video on demand application, the number of required servers is 35,
%   and the number of disk units is $68$.
%  \item[(A3)] Using the new version of \texttt{sendfile()} increases the server throughput by 51\%
%\end{itemize}
%\mq{metho-q2}{What is the difference between Examples (A1) to (A3)~?}%
%{(A1), (A3) are about a comparison; (A2)is about dimensioning}
%\end{ex}
%
%
%\begin{ex}{Quiz}
%\mylabel{ex-quiz1-kjn9} Consider the following cases and answer the
%next question.
%\begin{enumerate}
%  \item Design web server code that is efficient and fast.
%  \item Compare TCP-SACK versus TCP-new Reno for hand-held mobile devices.
%  \item Compare Windows 2000 Professional versus Linux.
%  \item Design a rate control for an internet audio application.
%  \item Compare various wireless MAC protocols.
%  \item Say how many servers a video on demand company needs to install.
%  \item Compare various compilers.
%  \item How many control processor blades should this Cisco router have ?
%  \item Compare various consensus algorithms.
%  \item Design bug-free code.
%  \item Design a server farm that will not crash when the load is high.
%  \item Design call center software that generates guaranteed revenue.
%  \item Size a hospital's information system.
%  \item What capacity is needed on an international data link~?
%  \item How many new servers, if any, should I install next quarter for my business application~?
%\end{enumerate}
%%\begin{maquestion}
%%\mylabel{mq-1} What examples fall in the scope of this lecture~?
%%\end{maquestion}
%\mq{metho-q1}{Say which examples require a detailed identification
%of the intensity of the workload.}{Examples 6, 8,13, 14, 15.}
%\end{ex}
%
%
%All of this requires knowing the system and its use.
%
%
%
%
%\mq{metho-kas6}{Among the examples in \exref{ex-quiz1-kjn9}, say
%which ones fall within the scope of this lecture~?}{All except 1,
%4, 10}


\section{Factors}\label{sec-hf} After defining goal, load and metric,
one needs to establish a list of \nt{factors}: these are elements in
the system or the load that affect the performance. One is tempted
to focus only on the factor of interest, however, it is important to
know all factors that may impact the performance measure, whether
these factors are desired or not.

\begin{ex}{Windows versus Linux, Continued} In
\cite{chen1995mpp}, Chen and co-authors consider the following
external factors: background activity; multiple users; network
activity. These were reduced to a minimum by shutting the network
down and allowing one single user. They also consider: the different
ways of handling idle periods in Windows and Limux, because they
affect the interpretation of measurements.
\end{ex}

\subsection{The Hidden Factor Paradox}\index{Hidden Factor Paradox}
Ignoring some hidden factors may invalidate the result of the
performance evaluation, as the next example shows.
\begin{ex}{TCP Throughput}
\fref{fig-stats-simpson}, left, plots the throughput achieved by a
mobile during a file transfer as a function of its velocity (speed).
It suggests that throughput increases with mobility. The right plot
shows the same data, but now the mobiles are separated in two
groups: one group (`s') is using a small socket buffer (4K Bytes),
whereas the second (`L') uses a larger socket buffer (16 K Bytes).
The conclusion is now inverted: throughput decreases with mobility.
The hidden factor influences the final result: all experiments with
low speed are for small socket buffer sizes. The socket buffer size
is a hidden factor.

\begin{figure}[htbp]
  \ifig{chapter5Ex6a}{0.5} \ifig{chapter5Ex6b}{0.5}
  \mycaption{Left: plot of throughput (in Mb/s) versus speed (in m/s) for a mobile node. Right: same plot, but showing socket buffer size;
  s = small buffer, L = large buffer.}
  \mylabel{fig-stats-simpson}
\end{figure}
\label{ex-simpson-tcp}
\end{ex}

Avoiding hidden factors may be done by proper randomization of
the experiments. On the example above, a proper design would
have distributed socket buffer sizes randomly with respect to
the speed.


However, this may not always be possible as some experimental
conditions may be imposed upon us; in such cases, all factors have
to be incorporated in the analysis. On \fref{fig-stats-simpson}, we
fitted a linear regression to the two figures, using the method
explained in \cref{ch-modfit}. The slope of the linear regression is
negative when we explicit the hidden factor, showing that mobility
decreases throughput.



The importance of hidden factors may be interpreted as our tendency
to confound cause and correlation \cite{pearl1988pri}. In
\fref{fig-stats-simpson}, left, the throughput is positively
correlated with the speed, but this may not be interpreted as a
causal relationship.


In conclusion at this point, knowing all factors is a tedious,
but necessary task. In particular, all factors should be
incorporated, whether you are interested in them or not
(factors that you are not interested in are called \nt{nuisance
factors}). This implies that you have to know your system well,
or be assisted by people who know it well.
 %\mq{q-stats-rl-sakl8}
% {Give a linear regression model for \fref{fig-stats-simpson}.}
% {Let $Y_{i}$ be the throughput of the $i$th data point, $s_i$ the speed, and $w_i=1$ when the
% window size is small, $w_i=2$ otherwise. A model is
% $Y_i= a_{w_i} + b_{w_i}  s_i + \epsilon_i$. The unknown parameter is $\vec{\beta}=(a_1, a_2, b_1,
% b_2)$ with $4$ degrees of freedom. The lines $y=a_i x + b_i$ are shown on \fref{fig-stats-simpson}, right.
% }
%
%In conclusion: before stating that some factor has a given impact on
%the overall performance, make sure that there is no hidden factor
%that plays a role.
%
%Confounding factors and factorial design. Point to \cref{ch-modfit}.
%
%
%The fact that hidden factors may cause a paradox is attributed to a
%problem with human intuition: we are tempted to believe that

\subsection{Simpson's Paradox} \nt{Simpson's reversal}, also known as
\nt{Simpson's paradox} is a well known case of the problem of hidden
factors, when the performance metric is a success probability.

\begin{ex}{TCP Throughput, continued}\label{ex-simpson}
We revisit the previous example, but are now interested only in
knowing whether a mobile can reach a throughput of at least 1.5
Mb/s, i.e.  we say that a mobile is successful if its throughput is
$\geq 1.5$Mb/s. We classify the mobiles as slow (speed $\leq 2 $m/s)
or fast (speed $> 2$m/s). We obtain the following result.
\begin{center}
 \begin{tabular}{|r|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
      & failure  & success &   & $\P($success$)$ \\ \hline
   \hline
   slow & 11 & 3 & 14 & 0.214 \\
   fast & 5 & 4 & 9 & 0.444 \\
   \hline
    &  16 & 7 & 23 &   \\
   \hline
 \end{tabular}
\end{center}
from where we conclude that fast mobiles have a higher success
probability than slow ones. Now introduce the nuisance parameter
``socket buffer size", i.e. we qualify the mobiles as `s' (small
buffer size) or `L' (large buffer size):
\begin{center}
 \begin{tabular}{|r|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    `s' mobiles  & failure  & success &   & $\P($success$)$ \\ \hline
   \hline
   slow & 10 & 1 & 11 & 0.091 \\
   fast & 1 & 0 & 1 & 0.00 \\
   \hline
    &  11 & 1 & 12 &   \\
   \hline
 \end{tabular}

 \begin{tabular}{|r|c|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    `L' mobiles  & failure  & success &   & $\P($success$)$ \\ \hline
   \hline
   slow & 1 & 2 & 3 & 0.667 \\
   fast & 4 & 4 & 8 & 0.500 \\
   \hline
    &  5 & 6 & 11 &   \\
   \hline
 \end{tabular}
 \end{center}

Now in both cases slow mobiles have a higher success probability
than fast ones, which is the correct answer. The former answer was
wrong because it ignored a hidden factor. This is known as
Simpsons's reversal.
\end{ex}

Simpsons' paradox can be formulated in general as follows
\cite{malinas}.
 Let $S$ denote the fact that the outcome of an
experiment is a success, and let $C$ be the factor of interest (in
the example, mobile speed). Let $N_i$, $i=1...k$ be binary hidden
factors (nuisance factors; in the example, there is only one, the
socket buffer size). Assume that the factor of interest has a
positive influence on the success rate, i.e.
 \be
 \P(S|C)  > \P(S|\bar{C}) \label{eq-sim-1}
 \ee
This may happen while, at the same time, the combination of the
factor of interest with the hidden factors $N_i$ has the opposite
effect: \be
 \P(S| C \mand N_i ) <  P(S| \bar{C} \mand N_i)
 \label{eq-sim-2}
\ee for all $ i=1...k$. As illustrated in Examples~\ref{ex-simpson}
and \ref{ex-simpson-tcp}, the reversal occurs when the effect of
hidden factors is large.

The fact that Simpson's reversal is a paradox is assumed to
originate in our (false) intuition that an average of factors leads
to an average of outcomes, i.e. we may (wrongly) assume that
\eref{eq-sim-1} is a weighted sum of \eref{eq-sim-2}.
  \begin{petit}
We do have weighted sums, but the weights are $\P(N_i|C)$ for the
left-handside in
 \eref{eq-sim-1} versus $\P(N_i|C)$ for the right-handside:
   \bearn
   \P(S|C) &=& \sum_i \P(S|C  \mand N_i)\P(N_i|C)\\
   \P(S|\bar{C}) &=& \sum_i \P(S|\bar{C}  \mand N_i)\P(N_i|\bar{C})\\
   \eearn
  \end{petit}

\section{Evaluation Methods} Once goal, load, metric and factors
are well defined, performance evaluation can then
proceed with a solution method, which usually
falls in one of the three cases below. Which
method to use depends on the nature of the
problem and the skills or taste of the evaluation
team.\doitemsep
\begin{itemize}
  \item \imp{Measurement} of the real system. Like in physics, it is hard to
measure
  without disturbing the system. Some special hardware devices (e.g.: optical splitters in network links)
  sometimes can prevent any disturbance. If, in contrast, measurements are taken by the system
  itself, the impact has to be analyzed carefully.
  Measurements are not always possible (eg. if the
 system does not exist yet).
  \item Discrete Event \imp{Simulation}: a simplified model of the system and its load are implemented in
  software. Time is simulated and often flows orders of magnitude more slowly than real
  time. The performance of interest is measured as on a real system, but measurement
  side-effects are usually not present. It is often easier than a measurement study, but not
  always. It is the most widespread method and is the object of
  \cref{ch-simul}.
  \item \imp{Analytical}: A mathematical model of the system is analyzed numerically. This is viewed
  by some as a special form of simulation. It is often much quicker than simulation,
  but sometimes wild assumptions need to be made in order for the numerical procedures to be
  applicable. Analytical methods are often used to gain insight
  during a development phase, or also to learn fundamental facts
  about a system, which we call ``patterns". We show in \cref{ch-queuing} how some performance analyses can
  be solved approximately in a very simple way, using bottleneck
  analysis. %In \cref{ch-fluid} we use fluid approximations.
\end{itemize}

\section{The Scientific Method} \mylabel{sec-scimed}
\nfs{    Exemple : regression vers la moyenne H: les mauvais se
resaisissent, les bons se rel\^{a}chent verif: simuler H et iid;
comparer }

The scientific method applies to any technical
work, not only to performance evaluation.
However, in the author's experience, lack of
scientific method is one prominent cause for
failed performance studies. In short, the
scientific method requires that you do not
believe in a conclusion unless it is thoroughly
tested.

\begin{ex}{Joe's kiosk}Joe's e-kiosk sells online videos to customers
equipped with smartphones. The system is made of
one servers and one 802.11 base station.
 Before deployment, performance evaluation tests are performed, as shown on
 \fref{metho-f1}. We see that the throughput
 reaches a maximum at around 8 transactions per
 second.

Joe concludes that the bottleneck is the wireless
LAN and decides to buy and install 2 more base
stations. After installation, the results are on
\fref{metho-f2}. Surprisingly, there is no
improvement. The conclusion that the wireless LAN
was the bottleneck was wrong.

Joe scratches his head and decides to go more
carefully about conclusions. Measurements are
taken on the wireless LAN; the number of
collisions is less than $0.1\%$, and the
utilization is below $5\%$. This confirms that
the wireless LAN is \emph{not} a bottleneck. Joe
makes the hypothesis that the bottleneck may be
on the server side. After doubling the amount of
real memory allocated to the server process, the
results are as shown on \fref{metho-f3}. This
confirms that real memory was the limiting
factor. \label{ex-joe}
 \end{ex}
 \begin{figure}[htbp]
 \begin{center}
 \subfigure[Initially]{
 \label{metho-f1}
 \Ifignc{joe-1}{0.3}{0.5}
 }
 \subfigure[With 2 additional base stations]{
 \label{metho-f2}
 \Ifignc{joe-2}{0.3}{0.5}
 }
 \subfigure[With additional server memory]{
 \label{metho-f3}
 \Ifignc{joe-3}{0.3}{0.5}
 }
 \end{center}
  \mycaption{Performance results for Joe's server. X-axis: offered load; Y-axis: achieved throughput, both
  in transactions per second. }
  \label{metho-f1f2f3}
 \end{figure}

A common pitfall is to draw conclusions from an
experiment that
 was not explicitly designed to validate these conclusion. The risk
 is that hidden factors might interfere, as illustrated by the previous example.
 Indeed, Joe concluded from the first experiment that
 the LAN performance would be improved by adding a base station;
 this may have been \emph{suggested} by the result of
 \fref{metho-f1}, but this is not sufficient. It is necessary to
 perform other experiments, designed to validate this potential conclusion, before
 making a final statement. Following Popper's philosophy of science \cite{popper1935lfe}, we claim
 that it is necessary for the performance analyst to take both roles :
(1) make tentative statements, and (2) design
experiments that try to invalidate them.

 \begin{ex}{ATM UBR better than ATM ABR}
 In \cite{402/LCA}, the authors evaluate whether the ATM-UBR protocol is better than ATM-ABR (both are alternative
 methods used to manage switches used in communication networks).
 They use a typical scientific method, by posing each potential
 conclusion as a hypothesis and designing experiments to try and
 invalidate them:
 \begin{quote}
 ABSTRACT. We compare the performance of ABR and UBR for providing high-speed network interconnection
services for TCP traffic. We test the hypothesis
that UBR with adequate buffering in the ATM
switches results in better overall goodput for
TCP traffic than explicit rate ABR for LAN
interconnection. This is shown to be true in a
wide selection of scenarios. Four phenomena that
may lead to bad ABR performance are identified
and we test whether each of these has a
significant impact on TCP goodput. This reveals
that the extra delay incurred in the ABR
end-systems and the overhead of RM cells account
for the difference in performance. We test
whether it is better to use ABR to push
congestion to the end-systems in a parking-lot
scenario or whether we can allow congestion to
occur in the network. Finally, we test whether
the presence of a ``multiplexing loop" causes
performance degradation for ABR and UBR. We find
our original hypothesis to be true in all cases.
We observe, however, that ABR is able to improve
performance when the buffering inside the ABR
part of the network is small compared to that
available at the ABR end-systems. We also see
that ABR allows the network to control fairness
between end-systems.
 \end{quote}
 \end{ex}

Other aspects of the scientific method are:
\noitemsep
\begin{itemize}
    \item Give an evaluation of the \imp{accuracy} of your
quantitative results. Consider the measured data
in \tref{metho-tab2}. There is a lot of
variability in them; saying that the average
response time is better with B than A is not
sufficient; it is necessary to give uncertainty
margins, or confidence intervals. Techniques for
this are discussed in \cref{ch-conf}.
    \item Make the results of your performance evaluation easily
\imp{reproducible}. This implies that all
assumptions are made explicit and documented.
\item Remove what can be removed. Often, at the end of a
performance evaluation study, many results are
found uninteresting; the right thing to do is to
remove such results, but this seems hard in
practice !
\end{itemize}
\doitemsep
%
%
%\paragraph{Dijkstra's Principle.} Like
%the scientific method, it is a common sense principle that
%applies to any technical activity. It is known under several
%equivalent forms, all of which can be summarized by:
%\imp{Remove what can be removed}.
%\begin{itemize}
%  \item (\nt{Occam}:) if two models explain some observations equally well, the simplest one is preferable
%  \item (\nt{Dijkstra}:) It is when you cannot remove a single piece that your design is complete.
%  \item (Common Sense:) Use the adequate level of sophistication.
%\end{itemize}
%For example, using a detailed simulation to answer \qref{metho-o5}
%would violate this principle.
%%\newpage
%
%


\section{Performance Patterns}
\mylabel{sec-pat} Performance evaluation is
simpler if the evaluator is aware of performance
\imp{patterns}, i.e. traits that are common to
many different settings.

\subsection{Bottlenecks}
A prominent pattern is \nt{bottlenecks}. In many
systems, the overall performance is dictated by
the behaviour of the weakest components, called
the bottlenecks.

\begin{ex}{Bottlenecks}\mylabel{ex-bn-p}
You are asked to evaluate the performance of an
information system. An application server can be
compiled with two options, A and B. An
experiments was done: ten test users (remote or
local) measured the time to complete
   a complex transaction on four days. On day 1, option A is used; on day 2, option B is. The results are
   in the table below.
\begin{center}
\begin{tabular}{|c|c|c|} \hline
  % after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
    & remote & local \\ \hline
  A & 123 & 43 \\
    & 189  & 38  \\
    & 99  & 49  \\
    & 167  & 37  \\
    & 177  & 44  \\ \hline

\end{tabular}
\begin{tabular}{|c|c|c|} \hline
  % after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
    & remote & local \\ \hline
    B & 107 & 62 \\ %\cline{col2-col3}
    & 179  & 69  \\
    & 199 & 56  \\
    & 103 & 47  \\
    & 178 & 71  \\ \hline
\end{tabular}
\label{metho-tab2}
\end{center}

The expert concluded that the performance for
remote users is independent of the choice of an
information system. We can criticize this finding
and instead do a bottleneck analysis. For remote
users, the bottleneck is the network access; the
compiler option has little impact. When the
bottleneck is removed, i.e. for local users,
option A is slightly better.
\end{ex}

%
%\begin{table}[htb] \mylabel{metho-tab2}
%\begin{center}
%\begin{tabular}{|c|c|c|} \hline
%  % after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
%    & remote & local \\ \hline
%  A & 123 & 43 \\
%    & 189  & 38  \\
%    & 99  & 49  \\
%    & 167  & 37  \\
%    & 177  & 44  \\ \hline
%    B & 107 & 62 \\ %\cline{col2-col3}
%    & 179  & 69  \\
%    & 199 & 56  \\
%    & 103 & 47  \\
%    & 178 & 71  \\ \hline
%\end{tabular}
%~~~~~~~~~
%\begin{tabular}{|c|c|c|} \hline
%  % after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
%    & remote & local \\ \hline
%  A & 141 & 75 \\
%    & 175  & 71  \\
%    & 192  & 62  \\
%    & 187  & 73  \\
%    & 125  & 58  \\ \hline
%    B & 201 & 90 \\ %\cline{col2-col3}
%    & 178  & 83  \\
%    & 193 & 102  \\
%    & 182 & 78  \\
%    & 186 & 92  \\ \hline
%\end{tabular}
%\end{center}
%\begin{tabular}{|c|l|c|c|} \hline
%  % after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
%user type & compile option & day & measured response time (sec) \\ \hline \hline
%  remote  &  A  & 1 &  123, 189, 99, 167, 177 \\ \hline
%  local   &  A  & 1  & 43, 38, 49, 37, 44 \\ \hline
%  remote   &  B  & 2   & 107, 179, 199, 103, 178 \\ \hline
%  local   &  B  & 2   & 62, 69, 56, 47, 71 \\ \hline
%  remote  &  A  & 181  & 141, 175, 192, 187, 125 \\ \hline
%  local   &  A  & 181  & 75, 71, 62, 73, 58 \\ \hline
%  remote  &  B  & 182  & 201, 178, 193, 182, 186 \\ \hline
%  local   &  B  & 182  & 90, 83, 102, 78, 92 \\ \hline
%\end{tabular}
  %\mycaption{Data for \exref{ex-bn-p}: measured performance of an information systems with
%  two compiler options A and B. Test users measured the time to complete
%   a complex transaction. Left: results of first tests. Right: results six months later.}
%\end{table}
Bottlenecks are the performance analysts' friend,
in the sense that they may considerably
\imp{simplify the performance evaluation}, as
illustrated next.
\begin{ex}{CPU model}
A detailed screening of a transaction system
shows that one transaction costs in average:
1'238'400 CPU instructions; 102.3 disk accesses
and 4 packets sent on the network. The processor
can handle $10^9$ instructions per second; the
disk can support $10^4$ accesses per second; the
network can support $10^4$ packets per second. We
would like to know how many transactions per
second the system can support.
%
% \mq{metho-o5}{Can you give a rough estimate ? If you want
%more accuracy, what would you study in detail ?}

The resource utilization per transaction per second
is: CPU:
$0.12\%$ -- disk: $1.02\%$ --network: $0.04\%$;
therefore the disk is the bottleneck. The
capacity of the system is determined by how many
transactions per second the disk can support, a
gross estimate is thus $\frac{100}{1.02}\approx
99$ transactions per second.

If we would like more accuracy, we would need to
model queuing at the disk, to see at which number
of transactions per seconds delays start becoming
large. A global queuing model of CPU, disk access
and network is probably not necessary.

In \sref{sec-botana} we study bottleneck analysis
for queuing systems in a systematic way.
\end{ex}

However, one should not be fooled by the apparent
simplicity of the previous example, as
bottlenecks are moving targets. They depend on
all parameters of the system and on the load: a
component may be a bottleneck in some conditions,
not in others. In particular, removing a
bottleneck may let some other bottleneck appear.

\begin{ex}{High Performance Web Sites}
In \cite{souders2007hpw}, the author discusses how to design
high performance web sites. He takes as performance metric
user's response time. He observes that modern web sites have
highly optimized backends, and therefore their bottleneck is at
the front end. A common bottleneck is DNS lookup; entirely
avoiding DNS lookups in web pages improves performances, but
reveals another bottleneck, namely, script parsing. This in
turn can be avoided by making scripts external to the web page,
but this will reveal yet another bottleneck, etc. The author
describes 14 possible components, any of which, if present, is
candidate for being the bottleneck, and suggests to remove all
of them. Doing so leaves as bottlenecks network access and
server CPU speed, which is desirable.
\end{ex}
\subsection{Congestion Collapse}
\nt{Congestion} occurs when the intensity of the
load exceeds system capacity (as determined by
the bottleneck). Any system, when subject to a
high enough load, will become congested: the only
way to prevent this is to limit the load, which
is often difficult or impossible. Therefore, it
is difficult to avoid congestion entirely.

In contrast, it is possible, and desirable, to
avoid \nt{congestion collapse}, which is defined
as a reduction in system utility, or revenue when
the load increases.

\begin{figure}[htbp]\begin{center}
        \insfignc{D31F1n}{0.45}\insfignc{cc}{0.45}
        \end{center}
        \mycaption{First panel: A network exhibiting congestion collapse if sources
        are greedy. Second panel: throughput per source $\la''$
        versus offered load per
        source $\la$, in Mb/s (plain line). Numbers are in Mb/s; the
        link capacity is $c=20$Mb/s for all
        links. Dotted line: ideal throughput with congestion but without congestion collapse.}
        \label{D31-f1n}
\end{figure}
\begin{ex}{Congestion Collapse}
Consider a ring network as in \fref{D31-f1n}
(such a topology is common, as it is a simple way
to provide resilience single link or node
failure). There are $I$ nodes and links, and
sources numbered $0, 1, ..., I-1$. At every node
there is one source, whose traffic uses the two
next downstream links (i.e. source $i$ uses links
$[(i+1) \bmod I] $ and $[(i+2) \bmod I]$). All
links and sources are identical.

Every source sends at a rate $\lambda$ and let
$c$ be the useful capacity of a link ($c$ and
$\lambda$ are in Mb/s).
 Let $\lambda'$ the rate achieved by
one source on its first hop, $\lambda''$ on its
second hop ($\la''$ is the throughput per
source). Since a source uses two links, we can
assume (in a simplified analysis) that, as along
as $\lambda < \frac{c}{2}$, all traffic is
carried by the network without loss, i.e.
 \ben
 \mif \la < \frac{c}{2}   \mthen \la' =\la" = \la
 \een

Assume now that sources are greedy and send as
much as they can, with a rate $\lambda >
\frac{c}{2}$. The network capacity is exceeded,
therefore there will be losses. We assume that
packet dropping is fair, i.e. the proportion of
dropped packets is the same for all flows at any
given link. The proportion of packets lost by one
source on its first hop is $\frac{\lambda-
\lambda'}{\lambda}$; on its second hop it is
$\frac{\lambda'- \lambda''}{\lambda'}$. By the
fair packet dropping assumption, those
proportions are equal, therefore \be
\frac{\la'}{\la}=\frac{\la''}{\la'}\label{eq-ex-cc1}\ee

Furthermore, we assume that links are fully
utilized when capacity is reached, i.e.
 \bearn
 \mif \la > \frac{c}{2} & & \mthen \la' +\la'' = c
  \eearn
We can solve for $\lambda'$ (a polynomial
equation of degree 2) and substitute $\la'$ in
\eref{eq-ex-cc1} to finally obtain the throughput
per source:
 \be
\lambda'' = c -\frac{\lambda}{2}\left( \sqrt{1 +
4 \frac{c}{\lambda}} -1 \right)
\label{eq-cc-ff099}
 \ee
\fref{D31-f1n} plots $\la''$ versus $\la$; it
suggests that $\la''\to 0$ as $\la \to \infty$.
We can verify this by using a Taylor expansion of
$\sqrt{1+u}$, for $u \to 0$ in
\eref{eq-cc-ff099}. We obtain
 \ben
 \lambda'' = \frac{c^2}{\lambda} \lp 1+ \eps(\la)\rp
 \een with $\limit{\la}{\infty}\eps(\la)=0$.
which shows that the limit of the achieved
throughput, when the offered load goes to
$+\infty$, is $0$. This is a clear case of
\imp{congestion collapse}.

\fref{D31-f1n} also illustrates the difference
between congestion and congestion collapse. The
dotted line represents the ideal throughput per
source if there would be congestion without
congestion collapse; this could be achieved by
employing a feedback mechanism to prevent sources
from sending more than $\frac{c}{2}$ (for example
by using TCP). \label{ex-cc}
\end{ex}
 Two common
causes for congestion collapse are:
\begin{enumerate}
    \item The system dedicates significant amounts of resources to jobs
    that will not complete, as in \fref{D31-f1n}, where packets are
    accepted on the first hop, which will
    eventually be dropped on the second hop. This
    is also known to occur on busy web sites or call
    centers due to customer \nt{impatience}: when response time gets large
    impatient customers drop requests before they
    complete.
    \item The service time per job increases as
    the load increases. This occurs for example
    when memory is paged to disk when the number
    of active processes increases.
\end{enumerate}

Congestion collapse is very common in complex
systems. It is a nuisance since it reduces the
total system utility below its capacity. Avoiding
congestion collapse is part of good system
design. A common solution to the problem is
\nt{admission control}, which consists in
rejecting jobs when there is a risk that system
capacity would be exceeded \cite{boudec05}.

\subsection{Competition Side
Effect}\index{Competition side effect} In many
systems the performance of one user influence
other users. This may cause an apparent paradox,
where putting more resources makes the
performance worse for some users. The root cause
is as follows: increasing some resources may
allow some users to increase their load, which
may in turn decrease the performance of competing
users. From the point of view of the user whose
performance is decreased, there is an apparent
paradox: resources were added to the system, with
an adverse effect.
\begin{figure}[htbp]
        \insfig{D31F1}{0.95}
        \mycaption{A simple network with two users showing the pattern of competition side effect.
        Increasing the capacity of
        link 5 worsens the performance of user 1.}
        \protect\label{D31-f1}
\end{figure}
\begin{ex}{Competing Users with Ideal Congestion Control} \fref{D31-f1}
shows a simple network with 2 users, 1 and 2,
sending traffic to destinations D1 and D2
respectively. Both users share a common link
$X-Y$.

Assume that the sources use some form of congestion control,
for example because they use the TCP protocol. The goal of
congestion control is to limit the source rates to the system
capacity while maintaining some fairness objective. We do not
discuss fairness in detail in this book, see for example
\cite{boudec05} for a quick tutorial; for simplicity, we may
assume here that congestion control has the effect of
maximizing the logarithms of the rates of the sources, subject
to the constraints that all link capacities are not exceeded
(this is called \nt{proportional fairness} and is approximately
what TCP implements). Let $x_1$ and $x_2$ be the rates achieved
by sources $1$ and $2$ respectively. With the numbers shown on
the figure, the constraints are $x_1 \leq 100$kb/s and $x_2\leq
10$kb/s (other constraints are redundant) so we will have $x_1
= 100$kb/s  and $x_2= 10$kb/s.

Assume now that we add resources to the system,
by increasing the capacity of link 5 (the weakest
link) from $c_5=10$kb/s to $c_5=100$kb/s. The
constraints are now
 \bearn
 x_1 & \leq &  100 \mbox{ kb/s }\\
  x_2 & \leq &  100 \mbox{ kb/s }\\
   x_1 + x_2& \leq &  110 \mbox{ kb/s }
 \eearn
 By symmetry, the rates allocated under
 proportional fairness are thus $x_1=x_2=55$kb/s.
We see that increasing capacity has resulted in a
decrease for source 1.
\end{ex}

The competition side effect pattern in the
previous example is a ``good" case, in the sense
that the decrease in performance for some users
is compensated by an increase for others. But
this is not always true; combined with the
ingredients of congestion collapse, the
competition side effect may result in a
performance decrease without any benefit for any
user (``put more, get less"), as shown in the
next example.

\begin{ex}{Competing Users without Congestion Control}
Consider \fref{D31-f1} again, but assume that
there is no congestion control (for example
because sources use UDP instead of TCP). Assume
that sources send as much as their access link
allows, i.e. source 1 sends at the rate of link 1
and source 2 at the rate of link 2.

Assume that we keep all rates as shown on the figure, except
for the rate of link 2, which we vary from $c_2=0$ to
$c_2=1000$kb/s. Define now the rates $x_1$ and $x_2$ as the
amounts of traffic that do reach the destinations.

If $c_2 \leq 10$kb/s, there is no loss and
$x_1=100$kb/s, $x_2=c_2$. If $c_2 > 10$kb/s,
there are losses at X. Assume losses are in
proportion to the offered traffic. Using the same
analysis as in \exref{ex-cc}, we obtain, for $c_2
> 10$:
 \bearn
 x_1 & = &110 \times \frac{100}{c_2 + 100}  \\
 x_2 & = &\min\lp 110 \times \frac{c_2}{c_2 +
 100}, 10\rp
 \eearn

\fref{fig-cc-x} plots the rates versus $c_2$. We
see that increasing $c_2$ beyond $10$kb/s makes
things worse for source 1, with no benefit for
source 2.
\begin{figure}[htbp]
        \insfig{ccx}{0.45}
        \mycaption{Achieved throughputs for the
        sources in
        \fref{D31-f1} versus $c_2$.}
        \protect\label{fig-cc-x}
\end{figure}
\end{ex}

\subsection{Latent Congestion Collapse}
\index{latent congestion collapse} \Ifig{musee}{0.9}{0.6} Many
complex systems have several potential bottlenecks, and may be
susceptible to congestion collapse. Removing a bottleneck (by
adding more resources) may reveal a congestion collapse,
resulting in worse performance. Before resources were added,
the system was protected from congestion collapse by the
bottleneck, which acted as implicit admission control. This
results in the ``put more, get less" paradox.


\begin{ex}{Museum audio guides} A museum offers free
audio guides to be downloaded on MP3 players.
Visitors put their MP3 player into docking
devices. The docking devices connect via a
wireless LAN to the museum server. Data transfer
from the docking device to the MP3 player is via
a USB connector. The system was tested with
different numbers of docking devices;
\fref{fig-museum-1} shows the download time
versus the number of docking devices in use.

The museum later decides to buy better docking
devices, with a faster USB connection between
device and MP3 player (the transfer rate is now
doubled). As expected, the download time is
smaller when the number $n$ of docking devices is
small, but, surprisingly, it is larger when $n
\geq 7$ (\fref{fig-museum-1}). What may have
happened ? It is known that the wireless LAN
access method is susceptible to congestion
collapse: when the offered load increases, packet
collisions become frequent and the time to
successfully transfer one packet becomes larger,
so the throughput decreases. We may conjecture
that improving the transfer speed between docking
device and MP3 player increases the load on the
wireless LAN. The congestion collapse was not
possible before because the low speed docking
devices acted as an (involuntary) access control
method.

We can verify this conjecture by plotting
throughput instead of download time, and
extending the first experiment to large values of
$n$. We see on \fref{fig-museum-2} that there is
indeed a reduction in throughput, at a point that
depends on the speed of the USB connection.
\end{ex}
\begin{figure}[htbp]
\begin{center}
\subfigure[Download Time
(seconds)]{\insfignc{museum-1}{0.45}\label{fig-museum-1}}
\subfigure[System Throughput
(Mb/s)]{\insfignc{museum-2}{0.45}\label{fig-museum-2}}
\mycaption{Illustration of latent congestion
collapse. Download time and System throughput as
a function of the number of docking devices, with
lower speed USB connections (o) with higher speed
USB connections (+).}
        \protect\label{fig-museum}
\end{center}
\end{figure}

\section{Review}
\subsection{Check-List}
\begin{sh}
\paragraph{Performance Evaluation Checklist}
 \begin{description}
  \item[PE1] \imp{Define your goal.} For example: dimension the system, find the overload
  behaviour; evaluate alternatives. Do you need a performance evaluation study  ? Aren't the results obvious ?
Are they too dependent on the input factors, which are arbitrary ?
  \item[PE2] \imp{Identify the factors.}
  What are all the factors ? are there external factors which need to be controlled ?
  \item[PE3] \imp{Define your metrics.}
  For example: response time, server occupancy, number of
  transactions per hour, Joule per
  Megabyte. Define not only what is measured but also under which condition
  or sampling method. If the metric is multidimensional, different
  metric values are not always comparable and there may not be
  a best metric value. However, there may be non dominated
  metric values.
  \item[PE4] \imp{Define the offered load.} How is it expressed: transactions per second, number of users, number of visits per hour ? Is it measured on a real system ? artificial load generated by a simulator, by a synthetic load
  generator ? load model in a theoretical model~?
  \item[PE5] \imp{Know your bottlenecks.} The performance often depends only on a small
  number of factors, often those whose utilization\index{utilization} (=~load/capacity) is high. Make sure what you are evaluating is one of them.
  \item [PE6] \imp{Know your system well.}
  Know the system you are evaluating and list all factors.
  Use evaluation tools that you know well. Know
  common performance patterns for your system.
\end{description}
\paragraph{Scientific Method Checklist}
\begin{description}
  \item[S1] \imp{Scientific Method} \newline
  \textbf{do} \{Define hypothesis; design experiments; validate \}
   \textbf{until} validation is OK
  \item[S2] Quantify the \imp{accuracy} of your results.
  \item[S3] Make your findings \imp{reproducible}; define your assumptions.
  \item[S4] \imp{Remove} what can be removed.
\end{description}
\end{sh}
\subsection{Review Questions}
\noitemsep \mq{metho-q1}{For each of the
following examples:
\begin{enumerate}
  \item Design web server code that is efficient and fast.
  \item Compare TCP-SACK versus TCP-new Reno for hand-held mobile devices.
  \item Compare Windows 2000 Professional versus Linux.
  \item Design a rate control for an internet audio application.
  \item Compare various wireless MAC protocols.
  \item Say how many servers a video on demand company needs to install.
  \item Compare various compilers.
  \item How many control processor blades should this Cisco router have ?
  \item Compare various consensus algorithms.
  \item Design bug-free code.
  \item Design a server farm that will not crash when the load is high.
  \item Design call center software that generates guaranteed revenue.
  \item Size a hospital's information system.
  \item What capacity is needed on an international data link~?
  \item How many new servers, if any, should I install next quarter for my business application~?
\end{enumerate}
say whether a detailed identification of the
intensity of the workload is required.}
 {Examples 6, 8, 13, 14, 15 are dimensioning exercises and require identification
 of the predicted workload intensity. Examples 1 and 10 are outside the scope of the book.
 Examples 11 and 12 are about avoiding congestion collapse.}

\mq{q-meth-rev1}{ Consider the following
scenarios.
\begin{enumerate}
  \item The web server used for online booking at the ``F\^{e}te des Vignerons" was so popular
  that it collapsed under the load, and was unavailable for several hours.
  \item Buffers were added to an operating system task, but the overall performance was degraded
  (instead of improved, as expected).
  \item The response time on a complex web server is determined
  primarily by the performance of the front end.
  \item When too many users are using the international link, the response time is poor
  \item When too many users are present on the wireless LAN, no one gets useful work done
  \item A traffic volume increase of 20\% caused traffic jams
  \item New parking facilities were created in the city center
  but free parking availability did not increase.
  %\item A new road was opened in the city center
\end{enumerate}
and the following patterns
\begin{itemize}
  \item[(a)] non-linearity of response time with respect to load
  \item[(b)] congestion collapse (useful work decreases as load increases)
  \item[(c)] performance is determined by bottleneck
\end{itemize}
Say which pattern is present in which scenario
}{1b; 2: perhaps a combination of b and c; 3c;
4a; 5b; 6b; 7c}
%8: maybe b, maybe other (see\cref{ch-patterns})}

%\mq{metho-lakjdn72}{Consider items
%11 and 12 in \exref{ex-quiz1-kjn9}. Which performance pattern do
%they correspond to~?}{Absence of congestion collapse.}
%\mq{metho-qs1}{Consider slides 298 and 299 in Nitin Vaidya's
%tutorial at Mobicom 2000 [Vaidya00-Mobicom1].
% The author studies
% the performance of TCP on a mobile ad-hoc network, as a function of speed (of mobile).
% What can you conclude from these two slides ?}
% {That mobility decreases throughput.}
% \mq{metho-qs2}{Consider slides 300--305 in Nitin Vaidya's tutorial at
% Mobicom 2000 [Vaidya00-Mobicom2].
% What can you conclude from these six slides ?}
% {That the previous conclusion was premature.}
% \mq{metho-qw8iu9}
% {What further measurements could be done to confirm the conclusion drawn in
%\exref{ex-bn-p}.}{Pose as assumption that the performance is a
%function of proportion of remote users and total load. Make
%measurements where these two factors take different values and
%analyze the dependency (for example, using a linear regression, see
%\cref{ch-modfit}).}
\mq{q-metho-akamai}{Read \cite{leighton2009ipi},
written by one of Akamai's founders. What topics in this chapter
does this illustrate ?}{(1) The performance bottleneck in internet
response time is the \emph{middle mile}, i.e. the intermediate
providers between web site provider and end-user ISP. (2)
performance metrics of interest are not only response time but also
reliability.}

%\section{Exercises}
%\m{p-1001}
