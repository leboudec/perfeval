\section{Parametric Estimation Theory}
\mylabel{sec-conf-mle}  In this appendix we give a large sample
theory which is used for some asymptotic confidence interval
computations in \cref{ch-conf} and for the general framework of
likelihood ratio tests of \cref{ch-tests}.

\subsection{The Parametric Estimation Framework. }

Consider a data set $x_i$, $i=1...,n$, which we view as the
realization of a stochastic system (in other words, the output
of a simulator). The framework of parametric estimation theory
consists in assuming that the parameters of the stochastic
system are well defined, but unknown to the observer, who tries
to estimate it as well as she can, using the data set.

We assume here that the model has a density of probability,
denoted with $f(x_1,...,x_n|\theta)$, where $\theta$ is the
parameter. It is also called the \nt{likelihood} of the
observed data. An \nt{estimator} of $\theta$ is any function
$T()$ of the observed data. A good estimator is one such that,
in average, $T(x_1,...,x_n)$ is ``close" to the true value
$\theta$.

\begin{ex}{i.i.d. Normal Data} Assume we can believe that our data is
iid and normal with mean $\mu$ and variance $\sigma^2$. The
likelihood is \be
 \frac{1}{\left(\sqrt{2
\pi}\sigma\right)^n}\exp\left(
-\frac{1}{2}\sum_{i=1}^n\frac{(x_i-\mu)^2}{\sigma^2} \right)
 \mylabel{eq-mle-niid}
 \ee
 and $\theta=(\mu,\sigma)$. An estimator of $\theta$ is
$\hat{\theta}=(\hat{\mu}_n, \hat{\sigma}_n)$ given by
\thref{theo-conf-normal}. Another, slightly different estimator
is $\hat{\theta}_1=(\hat{\mu}_n, s_n)$ given by
\thref{theo-conf-ln}.
\end{ex}

An estimator provides a random result: for every realization of
the data set, a different estimation is produced. The
``goodness" of an estimator is captured by the following
definitions. Here $\vX$ is the random data set,  $T(\vX)$ is
the estimator and $\E_{\theta}$ means the expectation when the
unknown but fixed parameter value is $\theta$.
\begin{itemize}
    \item \nt{Unbiased estimator}:
        $\E_{\theta}\left(T(\vX)\right)=\theta$. For
        example, the estimator $\hat{\sigma}^2_n$ of
        variance of a normal i.i.d. sample given by
        \thref{theo-conf-normal} is unbiased.
    \item \nt{Consistent family of estimators}:
        $\P_{\theta}(|T(\vX)-\theta|)> \epsilon)
        \rightarrow 0$ when the sample size $n$ goes to
        $\infty$. For example, the estimator $(\hat{\mu}_n,
        \hat{\sigma}_n)$ of \thref{theo-conf-normal} is
        consistent. This follows from the weak law of large
        numbers.
\end{itemize}

\subsection{Maximum Likelihood Estimator (MLE)\index{MLE}}
\label{sec-mle-defll} A commonly used method for
deriving estimators is that of \nt{Maximum
Likelihood}. The maximum likelihood estimator is
the value of $\theta$ that maximizes the
likelihood $f(x_1,...,x_n|\theta)$. This
definition makes sense if the maximum exists and
is unique, which is often true in practice. A
formal set of conditions is the regularity
condition in Definition~\ref{def-mle}.

In \sref{sec-mle-as}, we give a result that shows that the MLE
for an i.i.d. sample with finite variance is asymptotically
unbiased, i.e. the bias tends to $0$ as the sample size
increases. It is also consistent.

\begin{ex}{MLE for i.i.d. normal data}
Consider a sample $(x_1, ..., x_n)$ obtained from a normal
i.i.d. random vector $(X_1, ...,X_n)$. The likelihood is given
by \eref{eq-mle-niid}. We want to maximize it, where
 $x_1,...,x_n$ are given and $\mu, v=\sigma^2$ are the variables. For
 a given $v$, the maximum is reached when $\mu=\hat{\mu}_n=\frac{1}{n}\sum_{i=1}^n
 x_i$. Let $\mu$ have this value and find the value of $v$
 that maximizes the resulting expression, or to simplify, the log of it. We thus have to  maximize
 \be
-\frac{n}{2} \ln v -\frac{1}{2v} S_{x,x} + C
 \ee where
 $S_{x,x}\eqdef\sum_{i=1}^n(x-\hat{\mu}_n)^2$\index{1sxx@$S_{x,x}$} and $C$ is a constant with respect to $v$. This is a simple maximization problem in one variable $v$,
 which can be solved by computing the derivative. We find that
 there is a maximum for
 $v=\frac{S_{x,x}}{n}$. The maximum likelihood
 estimator of $(\mu,v)$ is thus precisely the estimator in
 \thref{theo-conf-ln}.
\end{ex}

We say that an estimation method \nt{invariant by
re-parametrization} if a different parametrization gives
essentially the same estimator. More precisely assume we have a
method which produces some estimator $T(\vX)$ for $\theta$.
 Assume we re-parametrize the problem by considering that the parameter is
 $\phi(\theta)$, where $\phi$ is some
 invertible mapping. For example, a normal i.i.d. sample can be parametrized by
 $\theta=(\mu, v)$ or by
 $\phi(\theta)=(\mu, \sigma)$, with $v=\sigma^2$.
%\mq{q-conf-sksak}{What is the mapping $\phi$ in this
%case~?}{$\phi(x,y)=(x,\sqrt{y})$ defined for $x \in \Reals$ and
%$y \geq 0$.}
The method is called invariant by re-parametrization if the
estimator of $\phi(\theta)$ is precisely $\phi(T(\vX))$.

The maximum likelihood method \emph{is} invariant by
re-parametrization. This is because the property of being a
maximum is invariant by re-parametrization. It is an important
property in our context, since the model is usually not given a
priori, but has to be invented by the performance analyst.

A method that provides an unbiased estimator cannot be invariant
by re-parametrization, in general. For example, $(\hat{\mu}_n,
\hat{\sigma}_n^2)$ of \thref{theo-conf-normal} is an unbiased
estimator of $(\mu, \sigma^2)$, but $(\hat{\mu}_n,
\hat{\sigma}_n)$ is a \imp{biased} estimator of $(\mu, \sigma)$
(because usually $\E(S)^2 \ne \E(S^2)$ except if $S$ is
non-random). Thus, the property of being unbiased is incompatible
with invariance by re-parametrization, and may thus be seen as an
inadequate requirement for an estimator.

Furthermore, maximum likelihood is also \nt{invariant by
reversible data transformation}, i.e. the MLE of $\theta$ is
the same, whether we look at the data or at a one to one
transform, independent of $\theta$. More precisely, assume
$\vX=(X_i)_{i=1...n}$ has a joint PDF $f_{\vX}(\vx|\theta)$,
and let $\vY=\varphi(\vX)$, with $\varphi$ a one-to-one,
differentiable mapping independent of $\theta$.

Take $\vX$ as data and estimate $\theta$; we have to maximize
$f_{\vX}(\vx|\theta)$ with respect to $\theta$, where
$\vx=(x_i)_{i=1...n}$ is the available data. If, instead, we
observe $y_i= \varphi(x_i)$ for all $i$, we have to maximize
 \ben
 f_Y(\vy) = \frac{1}{\abs{\varphi'(\vx)}}f_{\vX}(\vx)
 \een where $\abs{\varphi'(\vx)}$ is the absolute
 value of the determinant of the differential of
 $\varphi$ (i.e. the Jacobian matrix).

In particular, the MLE is invariant by \emph{re-scaling} of the
data. For example, if $Y_i$ is a log-normal sample (i.e. if
$Y_i=e^{X_i}$ and $X_i\sim$ iid $N_{\mu,\sigma^2}$), then the
MLE of the parameters $\mu,\theta$ can be obtained by
estimating the mean and standard deviation of $\ln(Y_i)$.

\subsection{Efficiency and Fisher Information}
The \nt{efficiency} of an estimator $T(\vX)$ of the parameter
$\theta$ is defined as the expected square error
$\E_{\theta}(\norm{T(\vX)-\theta}^2)$ (here we assume that
$\theta$ takes values in some space $\Theta$ where the norm is
defined). The efficiency that can be reached by an estimator is
captured by the concept of Fisher information, which we now
define.
%
Assume first to simplify that $\theta \in \Reals$. The
\nt{observed information} is defined by
$$
J(\theta)=-\frac{\partial^2 l(\theta)}{\partial \theta^2}
$$
where $l(\theta)$ is the \nt{log-likelihood}, defined by
$$
l(\theta) = \ln \mbox{lik}(\theta) = \ln f(x_1,...,x_n|\theta)
$$
The \nt{Fisher information}, or \nt{expected information} is
defined by
$$
I(\theta)
=\E_{\theta}(J(\theta))=\E_{\theta}\left(-\frac{\partial^2
l(\theta)}{\partial \theta^2}\right)
$$
For an i.i.d. model $X_1, ...,X_n$, $l(\theta)=\sum_i \ln
f_1(x_i|\theta)$ and thus $I(\theta)=n I_1(\theta)$, where
$I_1(\theta)$ is the Fisher information for a one point sample
$X_1$. The Cramer-Rao theorem says that the efficiency of any
\imp{unbiased} estimator is lower bounded by
$\frac{1}{I(\theta)}$. Further, under the conditions in
Definition~\ref{def-mle}, the MLE for an i.i.d. sample is
asymptotically maximally efficient, i.e.
$\E\left(\norm{T(\vX)-\theta}\right)/I(\theta)$ tends to $1$ as
the sample size goes to infinity.

In general, the parameter $\theta$ is multi-dimensional, i.e.,
varies in an open subset $\Theta$ of $\Reals^k$. Then $J$ and $I$
are symmetric matrices defined by
$$
[J(\theta)]_{i,j}=-\frac{\partial^2 l(\theta)}{\partial \theta_i
\partial \theta_j}
$$
and
$$
[I(\theta)]_{i,j}=-\E_{\theta}\left(\frac{\partial^2
l(\theta)}{\partial \theta_i \partial \theta_j}\right)
$$

The Cramer-Rao lower bound justifies the name of
``information". The variance of the MLE is of the order of the
Fisher information: the higher the information, the more the
sample tells us about the unknown parameter $\theta$. The
Fisher information is not the same as entropy, used in
information theory. There are some (complicated) relations --
see \cite[Chapter 16]{cover1991elements}.

In the next section we give a more accurate result, which can
be used to give approximate confidence intervals for large
sample sizes.

\section{Asymptotic Confidence Intervals} \mylabel{sec-mle-as}
Here we need to assume some regularity
conditions. Assume the sample comes from an
i.i.d. sequence of length $n$ and further, that
the following regularity conditions are met.
\begin{definition}\mylabel{def-mle}
Regularity Conditions for Maximum Likelihood Asymptotics
\cite{davison2003sm}.
\begin{enumerate}
  \item The set $\Theta$ of values of $\theta$ is compact
  (closed and bounded) and the true value $\theta_0$ is not on
  the boundary.
  \item (identifiability) for different values of $\theta$,
      the densities $f(\vx|\theta)$ are different.
  \item (regularity of derivatives) There exist a
      neighborhood $B$ of $\theta_0$ and a constant $K$
      such that for $\theta \in B$  and for all $i,j,k,n$~:
      $\frac{1}{n}\E_{\theta}(
      \left|\partial^3l_{\vX}(\theta)/\partial\theta_i\partial\theta_j\partial\theta_k\right|)\leq
      K$
  \item For
  $\theta \in B$ the Fisher information has
  full rank
  \item For $\theta \in B$ the interchanges of integration
      and derivation in $\int \frac{\partial
      f(\vx|\theta)}{\partial \theta_i}  dx= \frac{\partial
      }{\partial \theta_i} \int f(\vx|\theta) dx$ and $
      \int \frac{\partial^2 f(x|\theta)}{\partial \theta_i
      \partial \theta_j}  dx= \frac{\partial }{\partial
      \theta_i} \int \frac{\partial
      f({\vx}|\theta)}{\partial \theta_j} dx $ are valid
  \end{enumerate}
\end{definition}
The following theorem is proven in \cite{davison2003sm}.

\begin{shadethm}\mylabel{theo-mle}
Under the conditions in \dref{def-mle}, the MLE exists, converges
almost surely to the true value. Further $I(\theta)^{\frac{1}{2}}
(\hat{\theta} - \theta)$ converges in distribution towards a
standard normal distribution, as $n$ goes to infinity. It follows
that, asymptotically:
\begin{enumerate}
  \item the distribution of $\hat{\theta}-\theta$ can be approximated by
$N\left(0, I(\hat{\theta})^{-1}\right)$ or $N\left(0,
J(\hat{\theta})^{-1}\right)$
  \item the distribution of $2\left(l(\hat{\theta})-l(\theta)\right)$ can be approximated by  $\chi^2_k$  (where $k$ is the dimension
  of $\Theta$).
\end{enumerate}
\end{shadethm}
The quantity $2\left(l(\hat{\theta})-l(\theta)\right)$ is called
the \nt{likelihood ratio statistic}.

%\begin{petiteNote}
In the examples seen in this book, the regularity conditions
are always satisfied, as long as : the true value $\theta$ lies
within the interior of its domain, the derivatives of
$l(\theta)$ are smooth (for example, if the density
$f({\vx}|\theta)$ has derivatives at all orders) and the
matrices $J(\theta)$ and $I(\theta)$ have full rank. If the
regularity conditions hold, then we have an equivalent
definition of Fisher information:
$$
[I(\theta)]_{i,j}\eqdef-\E_{\theta}\left(\frac{\partial^2
l(\theta)}{\partial \theta_i \partial \theta_j}\right)=
\E_{\theta}\left(\frac{\partial l(\theta)}{\partial \theta_i}
\frac{\partial l(\theta)}{\partial \theta_j}\right)
$$
this follows from differentiating with respect to $\theta$ the
identity $\int f(x|\theta) d\!x=1$.

Item 2 is more approximate than item 1, but does not require to
compute the second derivative of the likelihood.

\thref{theo-mle} also holds for non-i.i.d. cases, as long as
the Fisher information goes to infinity with the sample size.
%\end{petiteNote}
%
%\mq{q-stat-asdasklksdk}{\thref{theo-mle} provides two asymptotic
%pivots. What are they~?}{$I(\theta)^{\frac{1}{2}} (\hat{\theta} -
%\theta)$ and $2\left(l(\hat{\theta})-l(\theta)\right)$.}

\begin{ex}{Fisher Information of Normal i.i.d. Model}
Assume $(X_i)_{i=1\ldots n}$ is i.i.d. normal with mean $\mu$
and variance $\sigma^2$. The observed information matrix is
computed from the likelihood function; we obtain:
$$J=\left(\begin{array}{cc}
 \frac{n}{\sigma^2} & \frac{2n}{\sigma^3}(\hat{\mu}_n-\mu)\\
  \frac{2n}{\sigma^3}(\hat{\mu}_n-\mu) & \frac{-n}{\sigma^2}+ \frac{3}{\sigma^4}\left(S_{xx}+n(\hat{\mu}_n-\mu)^2\right)
\end{array}\right)
$$
 and the expected information matrix (Fisher's information) is
$$I=\left(\begin{array}{cc}
 \frac{n}{\sigma^2} & 0\\
 0 & \frac{2n}{\sigma^2}
\end{array}\right)
$$
\end{ex}

The following corollary is used in practice. It follows
immediately from the theorem.
\begin{corollary}[Asymptotic Confidence Intervals] When $n$
is large, approximate confidence intervals can be obtained as
follows:\begin{enumerate}
    \item For the $i$th coordinate of $\theta$, the interval is: $\hat{\theta}_i \pm \eta \sqrt{\left[I(\hat{\theta})^{-1}\right]_{i,i}}$ or
     $\hat{\theta} \pm \eta \sqrt{\left[J(\hat{\theta})^{-1}\right]_{i,i}}$, where
    $N_{0,1}(\eta)=\frac{1+\gamma}{2}$ (for example, with $\gamma=0.95$,
$\eta=1.96$).
    \item If $\theta$ is in $\Reals$: the interval can be defined implicitly as $\{\theta\; : \; l(\hat{\theta})- \frac{\xi}{2} \leq l(\theta) \leq   l(\hat{\theta}) \}$,
    where $\chi^2_1( \xi)= \gamma$. For example, with $\gamma=0.95$, $\xi=3.84$.
\end{enumerate}
\mylabel{cor-conf-asfis}
\end{corollary}

\begin{ex}{Lazy Normal i.i.d.} Assume our data comes from an i.i.d.
normal model $X_i$, $i=1,...n$. We compare the exact confidence
interval for the mean (from \thref{theo-conf-normal}) to the
approximate ones given by the corollary.

The MLE of $(\mu,\sigma)$ is $(\hat{\mu}_n, s_n)$. The exact
confidence interval is \ben \hat{\mu}_n \pm \eta'
\frac{\hat{\sigma}_n}{\sqrt{n}}\een with $\hat{\sigma}^2_n=S_{x,x}
/ (n-1)$ and $t_{n-1}(\eta')=\frac{1+\gamma}{2}$.

Now we compute the approximate confidence interval obtained from
the Fisher information. We have
 \ben
 I(\mu,\sigma)^{-1}=\left(\begin{array}{cc}
 \frac{\sigma^2}{n} & 0\\
 0 & \frac{\sigma^2}{2n}
\end{array}\right)
 \een
thus the distribution of $(\mu-\hat{\mu}_n,\sigma-s_n)$ is
approximately normal with $0$ mean and covariance matrix
$\left(\begin{array}{cc}
 \frac{\sigma^2}{n} & 0\\
 0 & \frac{\sigma^2}{2n}
\end{array}\right)$. It follows that
$\mu-\hat{\mu}_n$ is approximately $N(0,\frac{s_n^2}{n})$, and an
approximate confidence interval is
 \ben
\hat{\mu}_n \pm \eta \frac{s_n}{\sqrt{n}}
 \een
with $s_n=s_{x,x}/n$ and $N_{0,1}(\eta)=\frac{1+\gamma}{2}$.

Thus the use of Fisher information gives the same asymptotic
interval for the mean as \thref{theo-conf-ln}. This is quite
general: the use of Fisher information is the generalization of
the large sample asymptotic of \thref{theo-conf-ln}.

We can also compare the approximate confidence interval for
$\sigma$. The exact interval is given by \thref{theo-conf-normal}:
with probability $\gamma$ we have
 \ben
  \frac{\xi_2}{n-1} \leq \frac{\hat{\sigma}^n_2}{\sigma^2} \leq \frac{\xi_1}{n-1}
 \een
 with $\chi^2_{n-1}(\xi_2)=\frac{1-\gamma}{2}$ and
 $\chi^2_{n-1}(\xi_1)=\frac{1+\gamma}{2}$. Thus an exact confidence
 interval for $\sigma$ is
  \be
\hat{\sigma}_n \; \left[\sqrt{\frac{n-1}{\xi_1}},\;
\sqrt{\frac{n-1}{\xi_2}}\right]
  \ee

With Fisher information, we have that $\sigma -s_n$ is
approximately $N_{0,\frac{\sigma^2}{2n}}$ Thus with probability
$\gamma$
 \ben |\sigma - s_n | \leq \eta \frac{\sigma}{\sqrt{2n}}
 \een
 with $N_{0,1}(\eta)=\frac{1+\gamma}{2}$ .

Divide by $\sigma$ and obtain, after some algebra, that with
probability $\gamma$:
 \ben
 \frac{1}{1 + \frac{\eta}{\sqrt{2n}}} \leq
 \frac{\sigma}{s_n} \leq
 \frac{1}{1 - \frac{\eta}{\sqrt{2n}}}
 \een
Taking into account that $s_n=\sqrt{\frac{n-1}{n}}\hat{\sigma}_n$,
we obtain the approximate confidence interval for $\sigma$
 \be
 \hat{\sigma}_n \;
 \left[
 \sqrt{\frac{n-1}{n}}\frac{1}{1 + \frac{\eta}{\sqrt{2n}}},
 \;
\sqrt{\frac{n-1}{n}}\frac{1}{1 - \frac{\eta}{\sqrt{2n}}},
 \right]
 \ee

 For $n=30,60,120$ and $\gamma=0.95$, the confidence intervals are
 as shown in \tref{tab-conf-nlz}, where we compare to exact values;
 the difference is negligible
 already for $n=30$.
\begin{table}
  \centering
  \begin{tabular}{|l||c|c|c|}\hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    $n$  & 30 & 60 & 120 \\ \hline
    Exact  & $0.7964 -  1.3443$& $0.8476 - 1.2197$& $0.8875 - 1.1454$\\
    Fisher & $0.7847 -  1.3162$& $0.8411 - 1.2077$& $0.8840 - 1.1401$\\
    \hline
  \end{tabular}
  \mycaption{Confidence Interval for $\sigma$ for an i.i.d., normal sample
of $n$ data points by exact method and asymptotic result with
Fisher information (\coref{cor-conf-asfis}). The values are the
confidence bounds for the ratio $\frac{\sigma}{\hat{\sigma}_n }$
where $\sigma$ is the true value and $\hat{\sigma}_n $ the
estimated standard deviation as in \thref{theo-conf-normal}.}
  \mylabel{tab-conf-nlz}
\end{table}
\end{ex}
\mq{stats-qmle200}
 {Which of the following are random variables: $\hat{\theta}$, $\theta$,
  $l(\theta)$, $l(\hat{\theta})$,  $J(\theta)$, $I(\theta)$, $J(\hat{\theta})$, $I(\hat{\theta})$~?
 }
 {In the classical,
  non Bayesian framework: $\hat{\theta}$, $l(\theta)$, $l(\hat{\theta})$, $J(\theta)$, $J(\hat{\theta})$,
  $I(\hat{\theta})$ are random variables; $\theta$ and $I(\theta)$ are non-random but unknown.
 }



 \section{Confidence Interval in Presence of Nuisance Parameters}
 \mylabel{sec-conf-nuisance}
 In many cases, the parameter has the form $\theta=(\mu, \nu)$, and we are interested only in $\mu$ (for example, for a
 normal model: the mean) while the remaining element $\nu$, which still needs to be estimated, is
 considered a nuisance (for example: the variance). In such cases, we can use the following
 theorem to find confidence intervals.
 \begin{shadethm}[\cite{davison2003sm}]\mylabel{theo-mle2}
 Under the conditions in \dref{def-mle}, assume that $\Theta = M \times
 N$, where $M, N$ are open subsets of $\Reals^p, \Reals^q$. Thus the parameter is $\theta=(\mu, \nu)$
 with $\mu \in M$ and $\nu \in N$ ($p$ is the ``dimension", or number of degrees of freedom, of
 $\mu$).

 For any $\mu$, let $\hat{\nu}_{\mu}$ be the
 solution to
 $$ l(\mu,\hat{\nu}_{\mu})=\max_{\nu}l(\mu, \nu)$$
  and define the \nt{profile log likelihood} $pl$ by
 $$
 pl(\mu)\eqdef\max_{\nu} l(\mu,\nu)= l(\mu, \hat{\nu}_{\mu})$$
Let $(\hat{\mu},\hat{\nu})$ be the MLE. If $(\mu,\nu)$ is the true
value of the parameter, the distribution of
$2\left(pl(\hat{\mu})-pl(\mu)\right)$ tends to $\chi^2_p$.

An approximate confidence region for $\mu$ at level $\gamma$ is
$$
\{\mu \in M: pl(\mu) \geq pl(\hat{\mu})-\frac{1}{2}\xi\}
$$
where $\chi^2_p(\xi)=\gamma$.
\end{shadethm} The theorem essentially says that we can find an approximate confidence
interval for the parameter of interest $\mu$ by computing the
profile log-likelihood for all values of $\mu$ around the
estimated value. The estimated value is the one that maximizes the
profile log-likelihood. The profile log likelihood is obtained by
fixing the parameter of interest $\mu$ to some arbitrary value and
compute the MLE for the other parameters. A confidence interval is
obtained implicitly as the set of values of $\mu$ for which the
profile log likelihood is close to the maximum. In practice, all
of this is done numerically.

\begin{ex}{Lazy Normal i.i.d. Revisited}
Consider the log of the data in \fref{fig-conf-ft1a}, which
appears to be normal. The model is $Y_i \sim i.i.d.
N_{\mu,\sigma^2}$ where $Y_i$ is the log of the data. Assume we
would like to compute a confidence interval for $\mu$ but are
too lazy to apply the exact student statistic in
\thref{theo-conf-normal}.

For any $\mu$, we estimate the nuisance parameter $\sigma$, by
maximizing the log-likelihood: \ben
l(\mu,\sigma)=-\frac{1}{2}\left(n \ln \sigma^2 +
\frac{1}{\sigma^2}\sum_i (Y_i-\mu)^2 \right)
 \een
It comes
$$
\hat{\sigma}^2_{\mu}=\frac{1}{n}\sum_i
(Y_i-\mu)^2=\frac{1}{n}S_{YY}+(\bar{Y}-\mu)^2
$$
and thus
 $$
 pl(\mu)\eqdef l(\mu,\hat{\sigma}_{\mu})=-\frac{n}{2}(\ln\hat{\sigma}_{\mu}^2 + 1)
 $$
On \fref{fig-conf-lazynorm} we plot $pl(\mu)$. We find
$\hat{\mu}=\input{lazyNormhatmu}$ as the point that maximizes
$pl(\mu)$. A $95\%$-confidence interval is obtained as the set
$\{pl(\mu) \geq pl(\hat{\mu})-\frac{1}{2}3.84\}$. We obtain the
interval $[\input{lazyNormmuci}]$. Compare to the exact confidence
interval obtained with \thref{theo-conf-normal}, which is equal to
$[\input{lazyNormmuciStudent}]$: the difference is negligible.

 \mq{q-mle2klkls9}
 {Find an analytical expression of the confidence interval obtained with the profile
 log likelihood for this example and compare with the exact interval.}
{The profile log likelihood method gives a confidence interval
defined by
$$
\frac{(\hat{\mu}-\mu)^2}{\frac{S_{YY}}{n}} \leq
e^{\frac{\eta}{n}}-1\approx \frac{\eta}{n}
$$
Let $t\eqdef\frac{\hat{\mu}-\mu}{\sqrt{\frac{S_{YY}}{n(n-1)}}}$
be the student statistic. The asymptotic confidence interval
can be rewritten as
 $$
t^2 \leq (n-1)(e^{\frac{\eta}{n}}-1)\approx \frac{\eta (n-1)}{n}
 $$
 An exact confidence interval is
 $$
t^2 \leq \xi^2
 $$
 where $\xi=t_{n-1}(1-\alpha/2)$. For large $n$, $\xi^2 \approx \eta$ and $\frac{n-1}{n} \approx
 1$ so the two intervals are equivalent.
 }
 \end{ex}
\begin{figure}[!htbp]
  \Ifig{lazyNorm}{0.6}{0.4}%
\nfs{fileTransferLogNormal.dat lazyn.m}%
  \mycaption{Profile log-likelihood for parameter $\mu$ of the log of the data in \fref{fig-conf-ft1a}.
  The confidence interval for $\mu$ is obtained by application of \thref{theo-mle2}.}
  \mylabel{fig-conf-lazynorm}
\end{figure}


\begin{ex}{Re-Scaling}
\mylabel{ex-conf-pl} Consider the data in \fref{fig-conf-ft1a},
which does not appear to be normal in natural scale, and for which
we would like to do a Box-Cox transformation. We would like a
confidence interval for the exponent of the transformation.

The transformed data is $Y_i= b_s (X_i)$, and the model now
assumes that $Y_i$ is i.i.d. $\sim N_{\mu,\sigma^2}$. We take
the unknown parameter to be  $\theta=(\mu, \sigma, s)$. The
distribution of $X_i$, under $\theta$ is:
  \ben f_{X_i}(x|\theta)=
  b_s'(x) f_{Y_i}\left(b_s(x)|\mu,\sigma\right)=x^{s-1}h(b_s(x)|\mu,\sigma^2)
  \een
where $h(x|\mu,\sigma^2)$ is the density of the normal
distribution with mean $\mu$ and variance $\sigma^2$.

The log-likelihood is
 \ben
  l(\mu,\sigma,s)= C - n \ln\sigma + \sum_i \left((s-1) \ln x_i -\frac{\left(b_s(x_i)-\mu\right)^2}{2 \sigma^2}\right)
 \een
 where $C$ is some constant (independent of the parameter). For a fixed $s$ it is maximized by the MLE for a Gaussian sample
 \ben\hat{\mu}_s=\frac{1}{n}\sum_i b_s(x_i)\een

  \ben \hat{\sigma}^2_s=\frac{1}{n}\sum_i
  \left(b_s(x_i)-\hat{\mu}\right)^2\een
We can use a numerical estimation to find the value of $s$ that
maximizes $l(\hat{\mu}_s,\hat{\sigma}_s,s)$;  see
\fref{fig-conf-pl} for a plot. The estimated value is
$\hat{s}=0.0041$, which gives $\hat{\mu}=1.5236$ and
$\hat{\sigma}=2.0563$.

We now give a confidence interval for $s$, using the asymptotic
result in \thref{theo-mle2}. A $95\%$ confidence interval is
readily obtained from \fref{fig-conf-pl}, which gives the interval
$[ -0.0782, 0.0841]$.

\mq{mle-qkla9}{Does the confidence interval justify the log
transformation~?} {Yes, since $0$ is in the interval.}

Alternatively, by \thref{theo-mle}, we can approximate the
distribution of $\hat{\theta}-\theta$ by a centered normal
distribution with covariance matrix $J(\hat{\theta})^{-1}$. After
some algebra, we compute the Fisher information matrix. We compute
the second derivative of the log-likelihood, and estimate the
Fisher information by the observed information (i.e. the value of
the second derivative at $\theta=\hat{\theta}$). We find:
$$
J=\left(\begin{array}{ccc}
  23.7 &        0  &  -77.1 \\
         0  &  47.3 &  -146.9 \\
   77.1 &  -146.9  &  1291.1
\end{array}\right)
$$
and
$$
J^{-1}=\left(\begin{array}{ccc}
 0.0605  &  0.0173  &  0.0056\\
    0.0173   & 0.0377  &  0.0053 \\
    0.0056  &  0.0053 &   0.0017
\end{array}\right)
$$
The last term of the matrix is an estimate of the variance of
$\hat{s}-s$. The 0.95 confidence interval obtained from a normal
approximation is $\hat{s} \pm 1.96 \sqrt{0.0017}=[-0.0770,
0.0852]$.
\end{ex}


\begin{figure}\Ifig{pl}{0.7}{0.3}%
\nfs{pl.m}%
 \mycaption{Profile log-likelihood for \exref{ex-conf-pl}, as a function
of the Box-Cox exponent $s$. The maximum likelihood estimator of
$s$ is the value that maximizes the profile log likelihood: a
confidence interval for $s$ is the set of $s$ for which the
profile log likelihood is below the horizontal dashed line.}
\mylabel{fig-conf-pl}
\end{figure}
