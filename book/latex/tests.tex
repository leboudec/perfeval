\begin{minipage}[b]{0.40\textwidth}
We use tests to decide whether some assertion on a model is
true or not, for example: does this data set come from a normal
distribution ? We have seen in \cref{ch-conf} that visual tests
may be used for such a purpose. Tests are meant to be a more
objective way to reach the same goal.

%
%There are two frameworks for tests: the classical, also called
%Neyman Pearson, and the Bayesian. We use the former, as it
%appears to be more frequently used with nested models.
\end{minipage}
%
\begin{minipage}[b]{0.60\textwidth}
\insfig{tests}{0.9}
%``No test can prove me right, a single
%test can prove me wrong".\footnote{Free adaptation of a
%sentence attributed to Albert Einstein}
\end{minipage}\hfill\\
%


Tests are often used in empirical sciences to draw conclusions from
noisy experiments. Though we use the same theories, our setting is a
bit different; we are concerned with the nested model setting, i.e.
we want to decide whether a simpler model is good enough, or whether
we do need a more sophisticated one. Here, the question is
asymmetric; if in doubt, we give preference to the simpler model --
this is the principle of parsimony. The Neyman Pearson framework is
well suited to such a setting, therefore we restrict ourselves to
it.

There is a large number of tests, and everyone can invent their
own (this is perhaps a symptom of the absence of a simple, non
equivocal optimality criterion). In practice though, likelihood
ratio tests are asymptotically optimal, in some sense, under
very large sets of assumptions. They are very general, easy to
use and even to develop; therefore, it is worth knowing them.
We often make use of Monte Carlo simulation to compute the
$p$-value of a test; this is sometimes brute force, but it
avoids spending too much time solving for analytical formulae.
We discuss ANOVA, as it is very simple when it applies. Last,
we also study robust tests, i.e. tests that make little
assumption about the distribution of the data.
%
\minitoc
%
\section{The Neyman Pearson Framework}
%
%
\subsection{The Null Hypothesis and The Alternative}
We are given a data sample $x_i$, $i=1,...,n$. We assume that
the sample is the output generated by some unknown model. We
consider two possible hypotheses about the model, $H_0$ and
$H_1$, and we would like to infer from the data which of the
two hypotheses is true. In the Neyman-Pearson framework, the
two hypotheses play different roles: $H_0$, the \nt{null
hypothesis}, is the conservative one. We do not want to reject
it unless we are fairly sure. $H_1$ is the \nt{alternative}
hypothesis.

We are most often interested in the \nt{nested model} setting:
the model is parameterized by some $\theta$ in some space
$\Theta$, and  $H_0 \eqdef ``\theta \in \Theta_0 "$ whereas
$H_1 \eqdef ``\theta \in \Theta \setminus \Theta_0 "$, where
$\Theta_0$ is a subset of $\Theta$.

In \exref{ex-vojex}, the model could be: all data points for
compiler option 0 [resp. 1] are generated as iid random
variables with some distribution $F_0$ [resp. $F_1$]. Then
$H_0$ is: ``$F_0=F_1$" and $H_1$ is ``$F_0$ and $F_1$ differ by
a shift in location". This is the model used by the Wilcoxon
Rank Sum test (see \exref{ex-npd-wil} for more details). Here
 $\Theta_0=\{(F_0, F_0), F_0 \mbox{ is a CDF}\}$ and
$\Theta=\{(F_0, F_1), F_0 \mbox{ is a CDF and} F_1(x)=F_0(x-m),
m\in \Reals\}$.

Another, commonly used model, for the same example could be:
all data points for compiler option 0 [resp. 1] are generated
as iid random variables with some normal distribution
$N_{\mu_0,\sigma^2}$ [resp. $N_{\mu_1,\sigma^2}$]. Then $H_0$
is: ``$\mu_0=\mu_1$" and $H_1$ is ``$\mu_0 \neq \mu_1$". This
is the model used by the so-called ``Analysis of variance" (see
\exref{ex-npd-anova} for more details). Here
$\Theta_0=\{(\mu_0,\mu_0, \sigma >0)\}$ and
$\Theta=\{(\mu_0,\mu_1, \sigma >0)\}$. Clearly this second
model makes more assumptions, and is to be taken with more
care.

%
\begin{ex}{Non Paired Data}
\mylabel{ex-vojex}\nfs{Data generated from normal random
variables $X_1,\ldots,X_n,Y_1,\ldots,Y_m$. $n=m=100$. $X$'s are
iid with the mean $\mu_0=0$ (likewise are $Y$'s with the mean
$\mu_1=0.7, 0.45$ or $0.1$). The variance is $\sigma^2=1$ for
both $X$'s and $Y$'s.}A simulation study compares the execution
time, on a log scale, with two compiler options. See
\fref{fig-examp451} for some data. We would like to test the
hypothesis that compiler option 0 is better than 1. For one
parameter set, the two series of data come from different
experiments.

We can compute a confidence interval for each of the compiler
options. The data looks normal, so we apply the student
statistic and find the confidence intervals shown on the
figure.

For parameter set 1, the confidence intervals are disjoint, so
it is clear that option 0 performs better. For parameter sets 2
and 3, the intervals are overlapping, so we cannot conclude at
this point.

We see here that confidence intervals may be used in some cases
for hypothesis testing, but not always. We study in this
chapter how tests can be used to disambiguate such cases.
\end{ex}
\begin{figure}[!htbp]\begin{center}
\subfigure[Parameter set 1]{\ifig{npd70}{0.3}}
\subfigure[Parameter set 2]{\ifig{npd45}{0.3}}
\subfigure[Parameter set 3]{\ifig{npd10}{0.3}}
\begin{tabular}{c|c|c}
\hline
 Parameter Set & Compiler Option 0 & Compiler Option 1 \\ \hline
 \hline
 1 & \input{npdConf70}\\ \hline
 2 & \input{npdConf45}\\ \hline
 3 & \input{npdConf10}\\ \hline
\end{tabular}
%\begin{tabular}{c|c|c|c}
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
% \hline
%  Parameter Set & 1 & 2 & 3 \\\hline \hline
%  Compiler Option 0 &  $[-0.0848; 0.2808]$ & $[-0.0945; 0.3475]$ & $[-0.1244; 0.2203]$ \\
%  Compiler Option 1 &  $[0.3679;  0.7334]$ & $[0.2575;  0.6647]$ & $[-0.2145; 0.1604]$ \\\hline
%\end{tabular}
  \end{center}
  \mycaption{Data for \exref{ex-vojex}. Top: Logarithm of execution time, on a log scale,
with two compiler options (o=option $0$, x=option $1$) for three
different parameter sets. Bottom: 95\% confidence intervals for the
means.}
  \label{fig-examp451}
\end{figure}

\subsection{Critical Region, Size and Power}
The \nt{critical region}, also called \nt{rejection region} $C$
of a test is a set of values of the tuple $(x_1,...,x_n)$ such
that if $(x_1,...,x_n)\in C$ we reject $H_0$, and otherwise we
accept $H_0$. The critical region entirely defines the
test\footnote{In all generality, one also should consider
randomized tests, whose output may be a random function of
$(x_1,...,x_n)$. See \cite{poor1994isd} for such tests. We do
not use them in our setting}.

The output of a test is thus a binary decision: ``accept
$H_0$", or ``reject $H_0$". The output depends on the data,
which is random, and may be wrong with some (hopefully small)
probability. We distinguish two types of errors
\begin{itemize}
    \item A \nt{type 1} error occurs if we reject $H_0$
        when $H_0$ is true
    \item Conversely, a \nt{type 2} error occurs if accept
        $H_0$ when $H_1$ is true.
\end{itemize}

The art of test development consists in minimizing both error
types. However, it is usually difficult to minimize two
objectives at a time. The maximum probability of a type 1
error, taken over all $\theta \in \Theta_0$ is called the
\nt{size} of the test. The \nt{power} function of the test is
the probability of rejection of $H_0$ as a function of $\theta
\in \Theta \setminus \Theta_0 $. Neyman-Pearson tests are
designed such that the size has a fixed, small value (typically
$5\%$, or $1\%$). Good tests (i.e. those in these lecture notes
and those used in Matlab) are designed so as to minimize,
exactly or approximately, the power, subject to a fixed size. A
test is said to be uniformly more powerful (UMP\index{UMP,
Uniformly More Powerful}) if, among all tests of same size, it
maximizes the power for every value of $\theta \in \Theta
\setminus \Theta_0$. UMP tests exist for few models, therefore
less restrictive requirements were developed (the reference for
these issues is \cite{lehmann2005tsh}).

It is important to be aware of the two types of errors, and of
the fact that the size of a test is just one facet. Assume we
use a UMP test of size $0.05$; it does not mean that the risk
of error is indeed $0.05$, or even is small. It simply means
that all other tests that have a risk of type 1 error bounded
by $0.05$ must have a risk of  type 2 error which is the same
or larger. Thus we may need to verify whether, for the data at
hand, the power is indeed large enough, though this is seldom
done in practice.

\begin{ex}{Comparison of Two Options, Reduction in
Run Time}\mylabel{ex-test-sgbd}\label{ex-test-paired} The
reduction in run time due to a new compiler option is given in
\fref{fig-conf-c2} on \pgref{fig-conf-c2}. Assume that we know
that the data comes from some iid $X_i$$\sim N_{\mu,\sigma^2}$.
This may be argued and will be discussed again, but it is
convenient to simplify the discussion here. We do not know
$\mu$ or $\sigma$.

We want to test $H_0$:~$\mu=0$ against $H_1$:~$\mu > 0$. Here
$\theta=(\mu, \sigma)$, $\Theta=[0,{\infty})\times (0,{\infty})
$ and $\Theta_0=\{0\}\times (0,{\infty})$. An intuitive
definition of a test is to reject $H_0$ if the sample mean is
large enough; if we rescale the sample mean by its estimated
standard deviation, this gives the rejection region
 \be
C = \left\{(x_1,...,x_n) \mst
 \frac{\bar{x}}{ s_n/\sqrt{n}}>c
 \right\}
 \ee
for some value of $c$ to be defined later and with, as usual
$
 \bar{x}=  \frac{1}{n}\sum_{i=1}^n X_i$ and $
 s_n^2 =\frac{1}{n}\sum_{i=1}^n \left(X_i-
 \bar{x}\right)^2$.

The size of the test is the maximum probability of $C$ for
$\theta \in \Theta_0$. We have
\ben
 \P\lp\left.\sqrt{n}\frac{\bar{x}}{ s_n}>c\right|\mu= 0, \sigma\rp\approx
 1-N_{0,1}(c)
 \een
where $N_{0,1}$ is the CDF of the standard gaussian
 distribution. Note that this is independent of $\sigma$ therefore
 \ben
 \alpha = \sup_{\sigma >0 }\lp 1-N_{0,1}(c)\rp =1-N_{0,1}(c)
 \een
  If we want a test size equal to $0.05$ we need
 to take $c=1.645$. For the data at hand the value of the test
 statistic is $\sqrt{n}\frac{\bar{x}}{_n}=5.05>c$ therefore we reject
 $H_0$ and decide that the mean is positive.


 The power
 function is
  \bear
  \beta(\mu,\sigma)&\eqdef &\P\lp\left.\sqrt{n}\frac{\bar{x}}{ s_n}>c\right|\mu, \sigma\rp
  \nonumber
\\
  &=&
  \P\lp\left.
\sqrt{n} \frac{\bar{x}-\mu}{s_n}>c-\sqrt{n}\frac{\mu}{
s_n}\right|\mu, \sigma
  \rp
  \nonumber
  \\
  & \approx & 1-N_{0,1}\lp c-\sqrt{n}\frac{\mu}{
\sigma}\rp \label{eq-ex-power}
  \eear

\begin{figure}
  % Requires \usepackage{graphicx}
  \insfig{power}{0.45}
  \mycaption{Power as a function of $\mu$ for \exref{ex-test-sgbd}.}\label{fig-tests-power}
\end{figure}
\fref{fig-tests-power} plots the power as a function of $\mu$ when
$c=1.645$ and for $\sigma$ replaced by its estimator value $s_n$ .
For $\mu$ close to the 0, the power is bad (i.e. the probability of
deciding $H_1$ is very small. This is unavoidable as
$\limit{\mu}{0}\beta(\mu, \sigma)=\alpha$.

For the data at hand, we estimate the power by setting
$\mu=\bar{x}$ and $\sigma=s_n$ in \eref{eq-ex-power}. For a
test size equal to $0.05$ (i.e. for $c=1.645$) we find
$0.9997$. The probability of a type 2 error (deciding for $H_0$
when $H_1$ is true) is thus approximately $0.0003$, a very
small value. If we pick as test size $\alpha=0.1\%$, we find
that the type 2 error probability is $2.5\%$.
\end{ex}
The previous example shows that the test size does not say
everything. On \fref{fig-examp451}, we see that there is a
``grey zone" (values of $\mu$ below, say, $15$) where the power
of the test is not large. If the true parameter is in the grey
zone, the probability of type 2 error may be large, i.e. it is
not improbable that the test will accept $H_0$ even when $H_1$
is true. It is important to keep this in mind: a test may
accept $H_0$ because it truly holds, but also because it is
unable to reject it. This is the fundamental asymmetry of the
Neyman-Pearson framework.

The power function can be used to decide on the size $\alpha$ of the
test, at least in theory, as illustrated next.
\begin{ex}{Optimal Test Size, Continuation of \exref{ex-test-sgbd}}
Say that we consider that a reduction in run time is negligible
if it is below $\mu^*$. We want that the probability of
deciding $H_0$ when the true value equal to $\mu^*$ or more is
similar to the size $\alpha$, i.e. we want to balance the two
types of errors. This gives the equations
 \bearn
 1-N_{0,1}\lp c^*\rp &=& \alpha
 \\
 1-N_{0,1}\lp c^*- \sqrt{n}\frac{\mu^*}{
s_n}\rp &= & 1-\alpha
 \eearn
 thus
 \ben N_{0,1}\lp c^*\rp + N_{0,1}\lp c^*- \sqrt{n}\frac{\mu^*}{
s_n}\rp= 1
 \een
 By symmetry of the gaussian PDF around its mean, we have
\ben \mif N_{0,1}(x)+N_{0,1}(y)=1 \mthen x+y = 0\een from where
we derive
 \ben c^* = \sqrt{n}\frac{\mu^*}{2 s_n}
 \een
The table below gives a few numerical examples, together with
the corresponding test size $\alpha^*=1-N_{0,1}\lp c^*\rp$.
\begin{center}
\begin{tabular}{ccc}
resolution $\mu^*$ & optimal threshold $c^*$ & size $\alpha^*$\\
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  10 & 0.97 & 0.17  \\
  20 & 1.93 & 0.02 \\
  40 & 3.87 & 5.38e-005 \\
  \hline
\end{tabular}
\end{center}
We see that if we care about validly detecting reductions in
run time as small as $\mu^*=10$ms, we should have a test size
of $17\%$ or more. In contrast, if the resolution $\mu^*$ is
$20$ms, then a test size of $2\%$ is
appropriate.\label{ex-tests-sgbd-2}
\end{ex}


\subsection{$p$-value of a Test.}
\label{sec-tests-pv} For many tests, the rejection region has
the form $\{T({\vx})
>m_0\}$, where ${\vx}$ is the observation, $T()$ some mapping, and
$m_0$is a parameter that depends on the size $\alpha$ of the
test. In \exref{ex-test-sgbd} we can take
$T({\vx})=\sqrt{n}\frac{\bar{x}}{s_n}$.


\begin{definition}
The $p$-value\index{p@$p$-value} of an observation ${\vx}$ is
\ben p^*({\vx}) = \sup_{\theta \in
\Theta_0}\P(T({\vX})>T({\vx})| \theta)\een
\end{definition}
In this formula, $\vX$ is a random vector that represents a
hypothetical replication of the experiment, whereas ${\vx}$ is
the data that we have observed.

The mapping $m\mapsto \sup_{\theta \in \Theta_0}\P(T({\vX})>m|
\theta)$ is monotonic nonincreasing, and usually decreasing.
Assuming the latter, we have the equivalence
 \ben p^*({\vx}) < \alpha \Leftrightarrow T({\vx}) > m_0
 \een
in other words, instead of comparing the test statistic
$T({\vx})$ against the threshold $m_0$, we can compare the
$p$-value against the test size $\alpha$:

\begin{center}
\imp{The test rejects $H_0$ when the $p$-value is smaller than the
test size $\alpha$.}
\end{center}

%
%
%More formally, call $\phi$ the mapping
%$$
%  \begin{array}{rcl}
%     [0,+\infty) &\rightarrow &[0,+\infty) \\
%     m & \mapsto &\sup_{\theta \in H_0 }\P_{\theta}\left\{
%      T(X) >m
%      \right\}
%  \end{array}
%$$
%Here $\theta$ is a model, and $\theta \in H_0$ means that the
%model satisfies the hypothesis $H_0$. Note that $\phi$ is
%wide-sense decreasing.  The $p$-value of an observation $x$ is
%$$p^*(x)\eqdef\phi(T(x))$$
%\begin{proposition}
%Assume that $\phi$ is strictly decreasing. The test is
%equivalent to: reject $H_0$ iff $p^*(x) < \alpha$, where
%$\alpha$ is the size of the test.
%\end{proposition}
%\begin{preuve} The rejection region is
%$$
%C\eqdef\{x:\;T(x) > m_0\}=\{x:\; \phi(T(X)) < \alpha\}=\{x:\;p^*(x) < \alpha\}
%$$ \end{preuve}
%
%The assumption that $\phi$ is strictly decreasing is usually
%true in practice. In other words, \imp{the test rejects $H_0$
%when the $p$-value is smaller than the test size $\alpha$}.

The interest of the $p$-value is that it gives more information
than just a binary answer. It is in fact the minimum test size
required to reject $H_0$. Very often, software packages return
$p$-values rather than hard decisions ($H_0$ or $H_1$).

%\mq{tests-qkj8}{What is the relation between $\alpha$, $\phi$
% and $m_0$~?}{$\alpha=\phi(m_0)$}

 \begin{exnn}{Continuation of \exref{ex-test-sgbd}}The $p$-value is
$ p^*=
 1-N_{0,1}\lp \frac{\sqrt{n}\bar{x}}{ s_n}\rp
$. We find $p^*= 2.2476e-007$ which is small, therefore we
reject $H_0$.
 \end{exnn}




\subsection{Tests Are Just Tests}
\label{sec-tajt} When using a test, it is important to make the
distinction between statistical significance and practical
relevance. Consider for example a situation, as in
\exref{ex-test-sgbd}, where we want to test whether a mean
$\mu$  satisfies $\mu=\mu_0 = 0$ or $\mu>\mu_0$. We estimate
the theoretical mean $\mu$ by the sample mean $\bar{x}$. It is
never the case that $\mu=\bar{x}$ exactly. A test is about
deciding whether the distance between $\mu_0$ and $\bar{x}$ can
be explained by the randomness of the data alone (in which case
we should decide that $\mu=\mu_0$), or by the fact that, truly,
$\mu>\mu_0$. Statistical significance means that, in a case
where we find $\bar{x}
> \mu_0$, we can conclude that there is a real difference, i.e.
$\mu>\mu_0$. Practical relevance means that the difference
$\mu- \mu_0$ is important for the system under consideration.
It may well happen that a difference is statistically
significant (e.g. with a very large data set) but practically
irrelevant, and vice versa (e.g. when the data set is small).

In some cases, tests can be avoided by the use of confidence
intervals. This applies to matching pairs as in
\exref{ex-test-sgbd}: a confidence interval for the mean can
readily be obtained by \thref{theo-conf-ln}. At level $0.05$,
the confidence interval is $[15.9, 36.2]$, so we can conclude
that $\mu>0$ (and more, we have a lower bound on $\mu$).


More generally, consider a generic model parameterized with
some $\theta\in \Theta\subset \Reals$. For testing
\begin{quote}
$\theta=\theta_0$ against $H_1$:~$\theta\neq \theta_0$
\end{quote}
we can take as rejection region
 \ben \abs{\hat{\theta}-\theta_0}> c\een

 If $\hat{\theta} \pm c$ is a confidence
 interval at level  $1-\alpha$, then the size of this test is precisely $\alpha$.
For such cases, we do not need to use tests, since we can
simply use confidence intervals as discussed in \cref{ch-conf}.
However, it is not always as simple, or even possible, to
reduce a test to the computation of confidence intervals, as
for example with unpaired data in \exref{ex-vojex} (though it
is possible to use confidence \emph{sets} rather than
confidence intervals).
%
%
% \mq{tests-q100}{(\exref{ex-vojex}) For one parameter set, the two data series come
%from different experiments. Assume, in contrast, they would
%come from \nt{matching pairs}, i.e. the $n$th data point for
%compiler options 0 and 1 come from the same transaction.  How
%could you decide whether compiler option 1 is better ?}{Compute
%the differences and a confidence interval for the median or the
%mean of the difference, and see if the confidence interval in
%entirely positive.}


%
%\subsection{A Glimpse of the Theory of Tests}
%\label{sec-theory-tests} \cite{lehmann2005tsh}
%
%
%
%A good test is one that, in addition, minimizes the power function,
%in some sense (see \sref{sec-theory-tests}).



\section{Likelihood Ratio Tests}
\mylabel{sec-mletests}\mylabel{sec-451} In this section we
introduce a generic framework, very frequently used for
constructing tests. It does not give UMP tests (as this is, in
general, not possible), but the tests are asymptotically UMP
(under the conditions of \thref{theo-mle-t}). We give the
application to simple tests for paired data and for goodness of
fit. Note that deciding which test is best is sometimes
controversial, and the best tests, in the sense of UMP, is not
always the likelihood ratio test \cite{lehmann2006lrt}; note
also that the issue of which criterion to use to decide that a
test is best is disputed \cite{perlman1999esn}. In our context,
likelihood ratio tests are appealing as they are simple and
generic.


\subsection{Definition of Likelihood Ratio Test}
\paragraph{Assumptions and Notation}
We assume the nested model setting, with  $H_0 \eqdef ``\theta
\in \Theta_0 "$ whereas $H_1 \eqdef ``\theta \in \Theta
\setminus \Theta_0 "$. For a given statistic (random variable)
${\vX}$ and value ${\vx}$ of ${\vX}$, define :

\begin{itemize}
  \item $l_{\vx}(\theta) \eqdef \ln f_{\vX}({\vx}|\theta)$
      where $f_{\vX}(.|\theta)$ is the probability density
      of the model, when the parameter is $\theta$.
  \item $l_{\vx}(H_0)=\sup_{\theta \in \Theta_0
      }l_{\vx}(\theta)$
   \item $l_{\vx}(H_1)=\sup_{\theta \in \Theta
       }l_{\vx}(\theta)$
\end{itemize}


For example, assume some data comes from an iid sequence of
normal RVs $\sim N(\mu,\sigma)$. We want to test $\mu=0$
versus $\mu \neq 0$. Here $\Theta=\{(\mu,\sigma>0)\}$ and
$\Theta_0=\{(0,\sigma>0)\}$.

If $H_0$ is true, then, approximately, the likelihood is
maximum for $\theta \in \Theta_0$ and thus
$l_{\vx}(H_0)=l_{\vx}(H_1)$. In the opposite case, the maximum
likelihood is probably reached at some $\theta \nin \Theta_0$
and thus $l_{\vx}(H_1)>l_{\vx}(H_0)$. This gives an idea for a
generic family of tests:
\begin{definition}
 \mylabel{def-lrs1} The likelihood ratio test is defined by the
 rejection region
$$C = \left\{l_{\vx}(H_1)-l_{\vx}(H_0) >k\right\}$$
where $k$ is chosen based on the required size of the test.
\end{definition}
The test statistic $l_{\vx}(H_1)-l_{\vx}(H_0)$ is called
\nt{likelihood ratio} for the two hypotheses $H_0$ and $H_1$.

Thus we reject $\theta \in \Theta_0$ when the likelihood ratio
statistic is large. The Neyman-Pearson lemma \cite[Section
6.3]{weber-c11} tells us that, in the simple case where
$\Theta_0$ and $\Theta_1$ contain only one value each, the
likelihood ratio test minimizes the probability of type 2
error. Most tests used in this lecture are actually likelihood
ratio tests. As we will see later, for large sample size, there
are simple, generic results for such tests.

There is a link with the theory of maximum likelihood estimation.
Under the conditions in \dref{def-mle}, define
\begin{itemize}
  \item $\hat{\theta}_0$ : the maximum likelihood estimator
      of $\theta$ when we restrict $\theta$ to be in
      $\Theta_0$
  \item $\hat{\theta}$ : the unrestricted maximum
      likelihood estimator of $\theta$
\end{itemize}
Then $l_{\vx}(H_0)=l_{\vx}(\hat{\theta}_0)$ and
$l_{\vx}(H_1)=l_{\vx}(\hat{\theta})$. In the rest of this
section and in the next two sections we show applications to
various settings.

 \mq{tests-q892}
 {Why can we be sure that $l_{\vx}(\hat{\theta}) - l_{\vx}(\hat{\theta}_0) \geq
  0$~?
 }
 {As long as the MLEs exist: by definition, $l_{\vx}\left(\hat{\theta}\right)
 \geq l_{\vx}(\theta)$ for any $\theta$.}

 \begin{exnn}{Continuation of
 \exref{ex-test-sgbd}, Compiler Options}\label{ex-test-sgbd-3}
We want to test $H_0$:~$\mu =0$ against $H_1$:~$\mu>0$.  The
log-likelihood of an observation is
$$
l_{\vx}(\mu,\sigma)=\frac{-n}{2}\ln\left(2 \pi \sigma^2\right)-\frac{1}{2
\sigma^2}\sum_i\left(x_i-\mu\right)^2
$$
and the likelihood ratio statistic is
 \bearn
l_{\vx}(H_1)-l_{\vx}(H_0)&=& \sup_{\mu\geq 0, \sigma >0}
l_{\vx}(\mu, \sigma)-\sup_{ \sigma >0}l_{\vx}(0,\sigma) = -n
\ln \frac{\hat{\sigma}_1}{\hat{\sigma}_0}
 \eearn
with
 \bearn
\hat{\sigma}_0^2&=&\frac{1}{n}\sum_i x_i^2\\
\hat{\sigma}_1^2&=&\frac{1}{n}\sum_i (x_i-\hat{\mu}_n^+)^2\\
\hat{\mu}_n^+&=& \max(\bar{x},0)
 \eearn
 The likelihood
ratio test has a rejection region of the form
$l_{\vx}(H_1)-l_{\vx}(H_0)>K$ for some constant $K$, which  is equivalent to
 \be \hat{\sigma}_1<k
\hat{\sigma}_0\label{eq-test-jjk}\ee
for some other constant $k$. In other words, we reject $H_0$
if the estimated variance under $H_1$ is small. Such a test is
called ``Analysis of Variance".

We can simplify the definition of the rejection region by
noting first that $\hat{\sigma}_1\leq \hat{\sigma}_0$, and thus
we must have $ k\leq 1$. Second, if $\bar{x}\geq 0$ then
\eref{eq-test-jjk} is equivalent to
$\sqrt{n}\frac{\bar{x}}{s_n}>c$ for some $c$. Third, if
$\bar{x}\leq 0$ then \eref{eq-test-jjk} is never true. In
summary, we have shown that this test is the same as the ad-hoc
test developed in \exref{ex-test-sgbd}.
\end{exnn}

\subsection{Student Test for Single Sample (or Paired Data)}
This test applies to a single sample of data, assumed to be
normal with unknown mean and variance. It can also be applied
to two paired samples, after computing the differences. It is
the two sided variant of \exref{ex-test-sgbd-3}. The model is:
$X_1, ..., X_n \sim$ iid $N_{\mu, \sigma^2}$ where $\mu$ and
$\sigma$ are not known. The hypotheses are:
\begin{quote}
$H_0$: $\mu=\mu_0$ against $H_1$:~$\mu\neq \mu_0$ \end{quote}
where $\mu_0$ is a fixed value. We compute the likelihood ratio
statistic and find after some algebra:
 \bearn%
l_{\vx}(H_1)-l_{\vx}(H_0)%&=&\max_{\mu,\sigma^2}\ln f_X(x|\mu,
%\sigma^2) -\max_{\sigma^2}\ln f_X(x|\mu_0, \sigma^2)\\
%%
%&=&\frac{n}{2}\left(-\ln \left(\sum_i (x_i-\bar{x})^2 \right)+
%\ln\left( \sum_i(x_i-\mu_0)^2\right)\right)\\
%%%
%&=&\frac{n}{2}\left(-\ln \left(\sum_i (x_i-\bar{x})^2 \right)+
%\ln( \sum_i(x_i-\bar{x})^2+ n(\bar{x}-\mu_0)^2)\right)\\
%%%
&=&\frac{n}{2}\ln\left(1+\frac{n(\bar{x}-\mu_0)^2}{\sum_i(x_i-\bar{x})^2}\right)
 \eearn
 Let $T({\vx})=\sqrt{n}\frac{\bar{x}-\mu_0}{\hat{\sigma}}$ be the student statistic
 (\thref{theo-conf-normal}),
 with $\hat{\sigma}^2=\frac{1}{n-1}\sum_i(x_i-\bar{x})^2$. We can
 write the likelihood ratio statistic as
 \be
l_{\vx}(H_1)-l_{\vx}(H_0) =
\frac{n}{2}\ln\left(1+\frac{T({\vx})^2}{n-1}\right)
 \ee
 which is an increasing function of $|T({\vx})|$. The rejection region thus has the form
\ben
 C = \left\{
\left|{T({\vx})}\right| > \eta
 \right\}
\een

We compute $\eta$ from the condition that the size of the test
is $\alpha$. Under $H_0$, $T({\vX})$ has a student distribution
$t_{n-1}$ (\thref{theo-conf-normal}). Thus
 \be \eta= t_{n-1}^{-1}\left(1-\frac{\alpha}{2}\right)
 \ee
 For example, for $\alpha=0.05$ and $n=100$, $\eta=1.98$.

 The $p$-value is
 \be
 p^*= 2(1-t_{n-1}(T({\vx})))
 \ee

\begin{ex}{Paired Data} This is a variant of \exref{ex-test-paired}. Consider again
the reduction in run time due to a new compiler option, as given
in \fref{fig-conf-c2} on \pgref{fig-conf-c2}. We want to test
whether the reduction is significant. We assume the data is iid
normal and use the student test:
\begin{quote}
$H_0$: $\mu=0$ against $H_1$:~$\mu\neq 0$ \end{quote} The test
statistic is $T({\vx})=5.05$, larger than $1.98$, so we reject
$H_0$. Alternatively, we can compute the $p$-value and obtain
$p^*= 1.80e-006$, which is small, so we reject $H_0$.
\end{ex}

%We can compare this test to the use of a confidence interval. A
%confidence interval for $\mu$ is (\thref{theo-conf-normal})
% \be
% \bar{x}\pm\eta \frac{\hat{\sigma}}{\sqrt{n}}
% \ee
%We could decide to reject $H_0$ iff $\mu_0$ is not in the
%confidence interval, i.e.
% \be
% \left|
%  \bar{x} -\mu_0
% \right|> \eta \frac{\hat{\sigma}}{\sqrt{n}}
% \ee
%which is exactly the same as the condition $T(x) >\eta$, which is
%the rejection condition of the student test. Thus there is
%equivalence between testing for the mean equal to $\mu_0$ and
%asking whether $\mu_0$ is in a confidence interval for the mean.
As argued in \sref{sec-tajt}, the Student test is equivalent to
confidence interval, so you do not need to use it. However, it
is very commonly used by others, so you still need to
understand what it does and when it is valid.


\subsection{The Simple Goodness of Fit Test}
\mylabel{sec-simple-gof} Assume we are given $n$ data points $x_1,
...,x_n$, assumed to be generated from an iid sequence, and we want
to verify whether their common distribution is a given distribution
$F()$. A traditional method is to compare the empirical histogram to
the theoretical one. Applying this idea gives the following
likelihood ratio test. We call it the \nt{simple goodness of fit
test} as the null hypothesis is for a given, fixed distribution
$F()$ (as opposed to a family of distributions, which would give a
\emph{composite} goodness of fit test).

To compute the empirical histogram, we partition the set of
values of ${\vX}$ into \nt{bins} $B_i$. Let $N_i=\sum_{k=1}^n
\ind{B_i}(X_k)$ (number of observation that fall in bin $B_i$)
and $q_i=\P\{X_1\in B_i\}$. If the data comes from the
distribution $F()$ the distribution of $\vec{N}$ is \nt{multinomial}
$M_{n,\vec{q}}$\index{$M_{n,\vec{q}}$}, i.e.
\be
 \P\left\{N_1=n_1, ...,N_k=n_k  \right\} =
 \frac{n!}{ n_1 ! ... n_k! }
 q_1^{n_1}...q_k^{n_k}
 \mylabel{eq-lik-multi}
\ee

The test is
\begin{quote}
$H_0$: $\vec{N}$ comes from the multinomial distribution $M_{n,\vec{q}}$

against

$H_1$: $\vec{N}$ comes from a multinomial distribution $M_{n,\vec{p}}$
for some arbitrary $\vec{p}$. \end{quote}

We now compute the likelihood ratio statistic. The parameter is
$\theta=\vec{p}$. Under $H_0$, there is only one possible value so
$\hat{\theta}_0=\vec{q}$. From \eref{eq-lik-multi}, the likelihood
is
 \be
 l_{\vec{x}}(\vec{p})= C + \sum_{i=1}^k n_i \ln(p_i)
\mylabel{eq-lik-gof}
 \ee
 where $n_i=\sum_{k=1}^n \ind{B_i}(x_k)$ and $C=\ln(n!)-\sum_{i=1}^k
 \ln(n_i!)$. $C$ is a constant and
 can be ignored in the rest. To find $\hat{\theta}$, we have to maximize \eref{eq-lik-gof} subject to the
 constraint $\sum_{i=1}^k p_i=1$. The function to maximize is
 concave in $p_i$, so we can find the maximum by the lagrangian
 technique. The lagrangian is
 \be
 L(\vec{p},\lambda)=\sum_{i=1}^k  n_i \ln(p_i) +\lambda (1
 -\sum_{i=1}^k p_i)
 \ee
The equations $\frac{\partial L}{\partial p_i}=0$ give $n_i=\lambda
p_i$. Consider first the case $n_i \neq 0$ for all $i$. We find
$\lambda$ by the constraint $\sum_{i=1}^k p_i=1$, which gives
$\lambda=n$ and thus $\hat{p_i}=\frac{n_i}{n}$. Finally, the
likelihood ratio statistic is
 \be \mylabel{eq-tests-lrsm}
 l_{\vec{x}}(H_1)-l_{\vec{x}}(H_0)=\sum_{i=1}^k n_i
 \ln\frac{n_i}{n q_i}
 \ee
In the case where $n_i=0$ for some $i$, the formula is the same if
we adopt the convention that, in \eref{eq-tests-lrsm}, the term $n_i
 \ln\frac{n_i}{n q_i}$ is replaced by $0$ whenever $n_i=0$.

We now compute the $p$-value. It is equal to
 \be
 \mylabel{eq-test-pv}
 \P\left(
\sum_{i=1}^k N_i
 \ln\frac{N_i}{n q_i} >\sum_{i=1}^k n_i
 \ln\frac{n_i}{n q_i}
 \right)
 \ee
 where $\vec{N}$ has the multinomial distribution $M_{n,\vec{q}}$.


For large $n$, we will see in \sref{sec-lrs-asymptotic} a simple
approximation for the $p$-value. If $n$ is not large, there is no
known closed form, but we can use Monte Carlo simulation as
discussed in \sref{sec-montecarlo}.

\begin{ex}{Mendel \cite{weber-c11}} Mendel crossed peas and classified the
results in 4 classes of peas $i=1,2,3,4$. If his genetic theory is
true, the probability that a pea belongs to class $i$ is $q_1=9/16,
q_2=q_3=3/16, q_4=1/16$. In one experiment, Mendel obtained $n=556$
peas, with $N_1=315, N_2=108, N_3=102$ and $N_4=31$. The test is
\begin{quote}
$H_0:$~``$\vec{q}=\vec{p}$" against $H_1:$~``$\vec{p}$ is arbitrary"
\end{quote} The test statistic is
 \be
 \sum_{i=1}^k n_i \ln\frac{n_i}{n q_i} =0.3092
 \ee
 We find the $p$-value by Monte-Carlo simulation
 (\exref{sec-montecarlo-pvalue}) and find $p=0.9191 \pm 0.0458$. The $p$-value is (very) large
 thus we
 accept $H_0$.
 \mylabel{ex-tests-mendel}
\end{ex}

\mq{q-tests-pval-nc-0}{Assume we compute the $p$-value of a
test by Monte Carlo simulation with $10 0$ replicates and find
an estimated $p$ equal to $0$. Can we say that the $p$-value is
small so we reject $H_0$~?}{A confidence interval for the
$p$-value at level $\gamma$ is given by \thref{theo-est-proba}
and is equal to $[0, \frac{3.689}{R}]$ where $R$ is the number
of replicates. We obtain that $p\leq 0.037$ at confidence
$\gamma=0.95$ thus we reject $H_0$.}

\section{ANOVA}
\label{sec-anova} In this section we cover a family of exact
tests when we can assume that the data is normal. It applies
primarily to cases with multiple, unpaired samples.

\subsection{Analysis of Variance (ANOVA) and $F$-tests}
\nt{Analysis of variance} (\nt{ANOVA}) is used when we can
assume that the data is a family of independent normal
variables, with an arbitrary family of means, but with common
variance. The goal is to test some property of the mean. The
name ANOVA is explained by \thref{theo-anova}.

ANOVA is found under many variants, and the basis is often
obscured by complex computations. All variants of ANOVA are
based on a single result, which we give next; they differ only
in the details of the linear operators $\Pi_M$ and $\Pi_{M_0}$
introduced below.
\paragraph{Assumptions and Notation for ANOVA}\noitemsep
\begin{itemize}
  \item The data is a collection of \emph{independent,
      normal} random variables $X_r$, here the index $r$ is
   in some finite set $R$ (with $\abs{R}=$ number of
   elements in
$R$).
  \item $X_r \sim N_{\mu_r, \sigma^2}$, i.e. all variables
      have the \emph{same variance} (this is pompously
      called ``homoscedasticity"\index{homoscedasticity}).
      The common variance is fixed but unknown.
  \item The means $\mu_r$ satisfy some linear constraints,
      i.e. we assume that $\vec{\mu}\eqdef(\mu_r)_{r \in
      R}\in M$, where $M$ is a linear subspace of
      $\Reals^R$. Let $k=\dim M$. The parameter of the
      model is $\theta=(\vec{\mu},\sigma)$ and the
      parameter space is $\Theta=M \times (0,+\infty)$
  \item We want to test the nested model $\vec{\mu} \in M_0$, where $M_0$ is a linear sub-space of
  $M$. Let $k_0=\dim M_0$.
  We have $\Theta_0=M_0 \times (0,+\infty)$.
  \item $\Pi_M$ [resp. $\Pi_{M_0}$] is the orthogonal
      projector on $M$ [resp. $M_0$]
\end{itemize}
\begin{exnn}{Non Paired Data}(Continuation of \exref{ex-vojex}) Consider the data for one parameter set.
The model is
 \be X_i = \mu_1 + \epsilon_{1,i}\;\; Y_j = \mu_2 + \epsilon_{2,j}
 \ee
 with $\epsilon_{i,j}\sim$ iid $N_{0,\sigma^2}$.
%
We can model the collection of variables as
$X_1,...,X_m,Y_1,...,Y_n$ thus $R=\lc1,...,m+n\rc$. We have
then
\begin{itemize}
  \item $M=\{(\mu_1,...\mu_1,\mu_2,...\mu_2), \mu_1 \in \Reals,
  \mu_2 \in \Reals\}$ and $k=2$
  \item $M_0=\{(\mu,...\mu,\mu,...\mu), \mu \in \Reals\}$ and $k_0=1$
  \item $\Pi_{M}(x_1,...,x_m,y_1,...,y_n)=(\bar{x}, ...,\bar{x},\bar{y},...,\bar{y})$, where
  $\bar{x}=(\sum_{i=1}^m x_i)/m$ and $\bar{y}=(\sum_{j=1}^n y_j)/n$.
  \item $\Pi_{M_0}(x_1,...,x_m,y_1,...,y_n)=(\bar{z}, ...,\bar{z},\bar{z},...,\bar{z})$, where
  $\bar{z}=(\sum_{i=1}^m x_i + \sum_{j=1}^n y_j)/(m+n)$.
\end{itemize}
This model is an instance of what is called ``one way ANOVA".
\end{exnn}

\begin{ex}{Network
Monitoring}\mylabel{ex-fruitflies}\nfs{fruitflies2.m} A network
monitoring experiment tries to detect changes in user behaviour
by measuring the number of servers inside the intranet accessed
by users. Three groups were measured, with 16 measurements in
each group. Only the average and standard deviations of the
numbers of accessed servers are available:
\begin{center}
 \begin{tabular}{c|c|c}
 \hline
 \emph{Group}& \emph{Mean number of remote servers}& \emph{standard
 deviation}\\\hline
 1 & 15.0625      & 3.2346 \\
 2 & 14.9375 &  3.5491  \\
 3 & 17.3125 & 3.5349\\\hline
\end{tabular}
 \end{center}
 (here the \nt{standard deviation} is $\sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i -
 \bar{x})^2}$).
The model is
 \be X_{i,j} = \mu_i + \epsilon_{i,j}\;\;  1\leq
 n_i \; i=1,...,k
 \ee
 with $\epsilon_{i,j}\sim$ iid $N_{0,\sigma^2}$.
 It is also called one-way ANOVA model (one way
 because there is one ``factor", index $i$). Here $i$ represents
 the group, and $j$ one measurement for one member of the group.
%
 The collection is
$X_r=X_{i,j}$ so $R=\{(i,j), i=1,...,k=3 \mand j=1,...,n_i\}$
and $\abs{R}=\sum_i n_i$. We have
\begin{itemize}
  \item $M=\{(\mu_{i,j}), \mst \mu_{i,j} = \mu_i,\; \forall
      i,j \}$; the dimension of $M$ is $k=3$.
  \item $M_0=\{(\mu_{i,j}) \mst \mu_{i,j} = \mu, \; \forall
      i,j\}$ and $k_0=1$.
  \item $\Pi_{M}(\vec{x})$ is the vector whose $(i,j)$th
      coordinate is independent of $j$ and is equal to
      $\bar{x}_{i.}\eqdef(\sum_{j=1}^{n_i}x_{i,j})/n_i$.
  \item $\Pi_{M_0}(\vec{x})$ is the vector whose
      coordinates are all identical and equal to the
      overall mean
      $\bar{x}_{..}\eqdef(\sum_{i,j}x_{i,j})/\abs{R}$.
\end{itemize}
\end{ex}


\begin{shadethm}[ANOVA]
\mylabel{theo-anova} Consider an ANOVA model as defined above.
The $p$-value of the likelihood ratio test of
$H_0$:``$\vec{\mu} \in M_0, \sigma >0 $"
 against $H_1$: ``$\vec{\mu} \in M \setminus M_0, \sigma >0 $" is
$p^*=1-F_{k-k_0, \abs{R}-k}(f)$ where $F_{m, n}()$ is the
Fisher distribution with degrees of
    freedom $m,n$, $\vec{x}$ is the dataset and
 \bear
  f&=&\frac{SS2/(k-k_0)}{SS1/(\abs{R}-k)}\label{eq-tests-f}\\
  SS2&=&\norm{\hat{\mu}-
  \hat{\mu}_0}^2\\
  SS1&=&\norm{\vec{x}-\hat{\mu}}^2\\
 \hat{\mu}_0 &=& \Pi_{M_0}(\vec{x})\\
 \hat{\mu} &=& \Pi_{M}(\vec{x})
 \eear
 (The norm is euclidian, i.e. $\norm{\vx}^2=\sum_r x_r^2$.)
%\begin{enumerate}
%  \item The Maximum Likelihood Estimators for both restricted and general models are
%given by \doitemsep
%\begin{itemize}
%  \item $\hat{\mu}_0 = \Pi_{M_0}(\vec{x})$,
%      $\hat{\sigma}_0^2=\frac{1}{\abs{R}}\|\vec{x}-\hat{\mu}_0\|^2$
%  \item $\hat{\mu} = \Pi_{M}(\vec{x})$,
%      $\hat{\sigma}^2=\frac{1}{\abs{R}}\|\vec{x}-\hat{\mu}\|^2$
%\end{itemize}
%\noitemsep where $\vec{x}$ is the value of the random variable $\vec{X}$.
% \item The likelihood ratio for the test of $H_0$: ``$\vec{\mu} \in M_0, \sigma >0 $"
% against $H_1$: ``$\vec{\mu} \in M \setminus M_0, \sigma >0 $" is
% $$
% -\frac{\abs{R}}{2}\ln\frac{SS1}{SS0}=\frac{\abs{R}}{2}\ln\left(1+\frac{SS2}{SS1}\right)
% $$where
% $$SS0\eqdef\|\vec{x}-\hat{\mu}_0\|^2=\abs{R} \hat{\sigma}_0^2=SS1+SS2$$
% $$SS1\eqdef\|\vec{x}-\hat{\mu}\|^2= \abs{R} \hat{\sigma}^2$$
% $$SS2\eqdef\|\hat{\mu}- \hat{\mu}_0\|^2=\abs{R}\times(\hat{\sigma}^2-\hat{\sigma}_0^2)$$
%  \item Define the test statistic $f$ by
%$$
%f\eqdef\frac{SS2/(k-k_0)}{SS1/(\abs{R}-k)}
%$$
%The distribution of $f$ (when we replace $\vec{x}$ by
%$\vec{X}$) under $H_0$ is $F_{k-k_0, \abs{R}-k}$.
%
%$F$ is often called the \nt{$F$-value} of the test.
%
%  \item The likelihood ratio test of size $\alpha$ rejects
%      $\vec{\mu} \in M_0$ when $f$ is large, i.e., when
%      $f>\eta$, where $F_{k-k_0,
%      \abs{R}-k}(\eta)=1-\alpha$. The $p$-value is
%      $p^*=1-F_{k-k_0, \abs{R}-k}(f)$.
%\end{enumerate}
\end{shadethm}
The theorem, the proof of which of which is a direct
application of the general ANOVA theorem
\ref{mle-theo-de-base}, can be understood as follows. The
maximum likelihood estimators under $H_0$ and $H_1$ are
obtained by orthogonal projection:
 \bearn
  \hat{\mu}_0 &=& \Pi_{M_0}(\vec{x}),\;
  \hat{\sigma}_0^2=\frac{1}{\abs{R}}\norm{\vx-\hat{\mu}_0}^2\\
  \hat{\mu} &=& \Pi_{M}(\vec{x}),\;
      \hat{\sigma}^2=\frac{1}{\abs{R}}\norm{\vx-\hat{\mu}}^2\\
\eearn
 The likelihood ratio statistic can be computed explicitly and
 is equal to $
 -\frac{\abs{R}}{2}\ln\frac{SS1}{SS0}=\frac{\abs{R}}{2}\ln\left(1+\frac{SS2}{SS1}\right)
 $, where $SS0\eqdef\|\vec{x}-\hat{\mu}_0\|^2=\abs{R}
 \hat{\sigma}_0^2=SS1+SS2$. Under $H_0$, the distribution of $f$, given by
 \eref{eq-tests-f}, is Fisher $F_{k-k_0, \abs{R}-k}$, therefore we can
 compute the $p$-value exactly.%
%
%\begin{enumerate}
% \item The likelihood ratio for the test of $H_0$: ``$\vec{\mu} \in M_0, \sigma >0 $"
% against $H_1$: ``$\vec{\mu} \in M \setminus M_0, \sigma >0 $" is
% $$
% -\frac{\abs{R}}{2}\ln\frac{SS1}{SS0}=\frac{\abs{R}}{2}\ln\left(1+\frac{SS2}{SS1}\right)
% $$where
% $$SS0\eqdef\|\vec{x}-\hat{\mu}_0\|^2=\abs{R} \hat{\sigma}_0^2=SS1+SS2$$
% $$SS1\eqdef\|\vec{x}-\hat{\mu}\|^2= \abs{R} \hat{\sigma}^2$$
% $$SS2\eqdef\|\hat{\mu}- \hat{\mu}_0\|^2=\abs{R}\times(\hat{\sigma}^2-\hat{\sigma}_0^2)$$
%  \item Define the test statistic $f$ by
%$$
%f\eqdef\frac{SS2/(k-k_0)}{SS1/(\abs{R}-k)}
%$$
%The distribution of $f$ (when we replace $\vec{x}$ by
%$\vec{X}$) under $H_0$ is $F_{k-k_0, \abs{R}-k}$.
%
%$F$ is often called the \nt{$F$-value} of the test.
%
%  \item The likelihood ratio test of size $\alpha$ rejects
%      $\vec{\mu} \in M_0$ when $f$ is large, i.e., when
%      $f>\eta$, where $F_{k-k_0,
%      \abs{R}-k}(\eta)=1-\alpha$. The $p$-value is
%      $p^*=1-F_{k-k_0, \abs{R}-k}(f)$.
%\end{enumerate}
%
\begin{figure}[!htbp]
  \insfig{anova}{0.5}
  \mycaption{Illustration of quantities in \thref{theo-anova}}
  \mylabel{fig-anova}
\end{figure}
The equality $SS0 = SS1 + SS2$ can be interpreted as a
decomposition of sum of squares, as follows. Consider
$\Theta_0$ as the base model, with $k_0$ dimensions for the
mean; we ask ourselves whether it is worth considering the more
complex model $\Theta$, which has $k >k_0$ dimensions for the
mean. From its definition, we can interpret those sums of
squares as follows.
\begin{itemize}
  \item $SS2$ is the sum of squares explained by the model
      $\Theta$, or explained variation.
  \item $SS1$ is the residual sum of squares
\item $SS0$ is the total sum of squares
\end{itemize}
  The likelihood ratio test accepts $\Theta$ when $SS2/SS1$ is large, i.e., when the
percentage of sum of squares $SS2/SS1$ (also called percentage
of variation) explained by the model $\Theta$ is high.

The dimensions are interpreted as degrees of
  freedom: $SS2$ (explained variation) is in the orthogonal of
      $M_0$ in $M$, with dimension $k-k_0=$ and the number of
      degrees of freedom for $SS2$ is $k-k_0$; $SS1$ (residual variation) is the square of
      the norm of a vector that is orthogonal to
      $M$ and the number of degrees of freedom
      for $SS1$ is $\abs{R}-k$.
This explains the name``ANOVA": the likelihood ratio statistic
depends only on estimators of variance. Note that this is very
specific of homoscedasticity.
\begin{table}
 \center
  \begin{tabular}{c|c|c|c|c|c}\hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Parameter Set 1 & SS & df & MS & F & Prob$>$F \\ \hline
    \input{npdAnova70}
    \hline
  \hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Parameter Set 2 & SS & df & MS & F & Prob$>$F \\ \hline
    \input{npdAnova45}
    \hline
  \hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Parameter Set 3 & SS & df & MS & F & Prob$>$F \\ \hline
    \input{npdAnova10}
    \hline
  \end{tabular}\\
  \mycaption{ANOVA Tests for \exref{ex-vojex} (Non Paired Data)}\mylabel{tab-test-anova1}
\end{table}
\begin{ex}{Application to \exref{ex-vojex}, Compiler Options}\mylabel{ex-npd-anova}
We assume homoscedasticity. We will check this hypothesis later
by applying the test in \sref{sec-test-chi2}. The theorem gives
the following computations:
\begin{itemize}
  \item
      $\hat{\mu}=(\bar{X},...,\bar{X},\bar{Y},...,\bar{Y})$
      and $\hat{\sigma}=\frac{1}{m+n}(\sum_i(X_i-\bar{X})^2
      +\sum_j(Y_j-\bar{Y})^2) $
  \item
      $\hat{\mu}_0=(\bar{Z},...,\bar{Z},\bar{Z},...,\bar{Z})$
      with $\bar{Z}=(m\bar{X}+n\bar{Y})/(m+n)$ and
      $\hat{\sigma}_0=\frac{1}{m+n}(\sum_i(X_i-\bar{Z})^2
      +\sum_j(Y_j-\bar{Z})^2)$
  \item $SS1 = \sum_i(X_i-\bar{X})^2   +\sum_j(Y_j-\bar{Y})^2)=S_{XX}+S_{YY}$
  \item $SS2 = m (\bar{Z}-\bar{X})^2 + n (\bar{Z}-\bar{Y})^2 = (\bar{X}-\bar{Y})^2/(1/m+ 1/n)$
  \item the $f$ value is $\frac{SS2}{SS1/(m+n-2)}$.
\end{itemize}

The ANOVA tables for parameter sets 1 to 3 are given in
\tref{tab-test-anova1}.  The F-test rejects the hypothesis of same
mean for parameter sets 1 and 2, and accepts it for parameter set
3. The software used to produce this example uses the following
terminology:\begin{itemize}
    \item SS2: ``Columns" (explained variation, variation between columns, or between
    groups)
    \item SS1: ``Error" (residual variation, unexplained
    variation)
    \item SS0: ``Total" (total variation)
\end{itemize}\label{ex-anova-npd48}
\end{ex}


 \mq{q-kdskdsakljdsaf90}{Compare to the confidence intervals given in the introduction.}{For parameter set 1, the conclusion is
 the same as with confidence interval.
For parameter sets 2 and 3, confidence intervals did not allow one
to conclude. ANOVA disambiguates these
 two cases.}

\mq{q-sslksdmsdl}{What are SS0, SS1 and SS2 for parameter set
1~?}{The column ``SS" gives, from top to bottom: SS2, SS1 and
SS0.}

%\paragraph{Interpretation.}
%
%Item 2 in the theorem justifies the name ``ANOVA": the
%likelihood ratio statistic depends only on estimators of
%variance. Note that this is very specific of homoscedasticity.
%
%
%The equality
%$$SS0 = SS1 + SS2$$
%can be interpreted as a decomposition of sum of squares, as follows.
%Consider $\Theta_0$ as the base model, with $k_0$ dimensions for the mean; we ask ourselves
%whether it is worth considering the more complex model $\Theta$, which has $k >k_0$ dimensions
%for the mean. From its definition, we can interpret those some of squares as follows.
%\begin{itemize}
%  \item $SS2$ is the sum of squares
%explained by the model $\Theta$, or explained variation.
%  \item $SS1$ is the residual sum of
%squares
%\item $SS0$ is the total sum of squares
%\end{itemize}
%  The likelihood ratio test accepts $\Theta$ when $SS2/SS1$ is large, i.e., when the
%percentage of sum of squares $SS2/SS1$ (also called percentage of variation) explained by the
%model $\Theta$ is high.
%
%The dimensions are interpreted as degrees of freedom:
%\begin{itemize}
%  \item $SS2$ (explained variation) is in the orthogonal of $M_0$ in $M$, with dimension
%  $k-k_0$: the number of degrees of freedom for $SS2$ is $k-k_0$
%  \item $SS1$ (residual variation) in in the orthogonal of
%      $M$ in $\Reals^R$. The number of degrees of freedom
%      for $SS1$ is $\abs{R}-k$
%\end{itemize}
%

\begin{exnn}{Network Monitoring}The numerical solution of
\exref{ex-fruitflies} is shown in the table below. Thus we
accept $H_0$, namely, the three measured groups are similar,
though the evidence is not strong.
 \begin{center}
  \begin{tabular}{c|c|c|c|c|c}\hline
    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
     Source & SS & df & MS & F & Prob$>$F \\ \hline
% Columns &30.427&2 &15.213 &0.0628 &0.9392\\
% Errors &17449.92&72  & 242.36 & & \\
% total & 17480.35& 74&&&  \\  \hline
Columns &  57.1667 & 2 &  28.5833 &   2.4118 &   0.1012 \\
Errors & 533.3140 & 45 &  11.8514 &   &   \\
Total & 590.4807 & 47 &  &   &   \\
  \end{tabular}
\end{center}
\mq{anova-qio86}
 {Write down the expressions of MLEs, $SS1$, $SS2$ and the $F$-value.}
 {\begin{itemize}
  \item $\hat{\mu}$ is the vector whose
 $(i,j)$th coordinate is independent of $j$ and is equal to
  $\bar{X}_{i.}\eqdef\sum_{j=1}^{n_i}X_{i,j}/n_i$.
  \item $SS1=\sum_{i,j}(X_{i,j}-\bar{X}_{i.})^2$
  \item $\hat{\sigma}^2=\frac{1}{\abs{R}}SS1$
  \item $\hat{\mu}_0$ is the vector whose coordinates are
      all identical and equal to the overall mean
      $\bar{X}_{..}\eqdef(\sum_{i,j}X_{i,j})/\abs{R}$
  \item $SS2=\sum_i n_i (\bar{X}_{i.}-\bar{X}_{..})^2$
  \item $SS0=SS1 + SS2$
  \item $\hat{\sigma}_0^2=\frac{1}{\abs{R}}SS0$
  \item $F=SS2 (\abs{R}-k)/\lb SS1(k-1)\rb$
\end{itemize}
 }
\end{exnn}

\paragraph{Student test as special case of ANOVA. } In the special case where $k-k_0=1$ (as in
\exref{ex-vojex}) the $F$-statistic is the square of a student
statistic, and a student test could be used instead. This is
used by some statistics packages.

\paragraph{Testing for Specific Values} By an additive change of
variable, we can extend the ANOVA framework to the case where
$M_0 \subset M$ are affine (instead of linear) varieties of
$\Reals^R$.  This includes testing for a specific value.
 For example, assume we have the model
 \be
 X_{i,j}=\mu_i + \epsilon_{i,j}
 \ee
 with $\epsilon_{i,j}\sim$ iid $N_{0,\sigma^2}$. We want to test
  \begin{quote}
  $H_0$:~``$\mu_i=\mu_0$ for all $i$" against $H_1$:~``$\mu_i$
  unconstrained"
  \end{quote}
We change model by letting $X'_{i,j}=X_{i,j}-\mu_0$ and we are
back to the ANOVA framework.

\subsection{Testing for a Common Variance}
\mylabel{sec-test-chi2} We often need to verify that the common
variance assumption holds. Here too, a likelihood ratio test
gives the answer. In the general case, the $p$-value of the
test cannot be computed in closed form, so we use either Monte
Carlo simulation or an asymptotic approximation. When the
number of groups is 2, there is a closed form using the Fisher
distribution. %We give the likelihood ratio test, as it derives
%from the general theory; see alo

We are given a data set with $I$ groups $x_{i,j}$, $i=1,...,I$,
$j=1,...,n_i$; the total number of samples is $n=\sum_{j=1}^I
n_j$. We assume that it is a realization of the model $X_{i,j}
\sim$ iid $N_{\mu_i,\sigma_i^2} $. We assume that the normal
assumption holds and want to test
\begin{quotation} $H_0$: $\sigma_i=\sigma>0$ for all $i$
against $H_1$: $\sigma_i>0$ \end{quotation}
%
  \begin{shadethm}[Testing for Common Variance]
 The likelihood ratio statistic $\ell$ of the test of common variance
 under the hypothesis above is given by
   \bear
    2 \ell & = &  n \ln(s^2) - \sum_{i=1}^I n_i
 \ln(s^2_i)  \label{lrs-pv-32}\\
   \mwith
   \hat{\mu}_i & \eqdef &\frac{1}{n_i}\sum_{j=1}^I x_{i,j}, \;
   s_i^2  \eqdef  \frac{1}{n_i}\sum_{j=1}^I
   (x_{i,j}-\hat{\mu}_i)^2,\nonumber \\
   s^2  &\eqdef&  \frac{1}{n}\sum_{i=1}^I\sum_{j=1}^{n_i}
 (x_{i,j}-\hat{\mu}_i)^2 = \sum_{i=1}^I \frac{n_i}{n}
 s_i^2 \nonumber
   \eear
 The test rejects $H_0$ when $\ell$ is large. The $p$-value is
  \be p  =  \P\lp n \log \sum_{i=1}^I Z_i -
\sum_{i=1}^I n_i \log
 Z_i > 2 \ell+n \log n -\sum_{i=1}^I n_i \log n_i\rp \label{eq-tests-cvdsmc}\ee
where $Z_i$ are independent random variables, $Z_i \sim
\chi^2_{n_i-1}$ and
 $Z=\sum_{i=1}^I \frac{n_i}{n}Z_i$. The $p$-value can be
 computed by Monte Carlo simulation. When $n$ is large:
 \be
 p \approx 1-\chi^2_{I-1}(2\ell) \label{eq-tests-dfjsjdsf}
 \ee

In the special case $I=2$, we can replace the statistic $\ell$
by
  \bearn
  f & \eqdef &\frac{\hat{\sigma}_1^2}{\hat{\sigma}_2^2}\;
\mwith \hat{\sigma}_i^2 \eqdef  \frac{1}{n_i-1}\sum_{j=1}^I
(x_{i,j}-\hat{\mu}_i)^2
  \eearn and the distribution of $f$ under $H_0$ is Fisher $F_{n_1-1,n_2-1}$.
The test at size $\alpha$ rejects $H_0$ when $f<\eta$ or
$f>\xi$ with $F_{n_1-1,n_2-1}(\eta)=\alpha/2$,
$F_{n_1-1,n_2-1}(\xi)=1-\alpha/2$. The $p$-value is
  \be
  p = F_{n_1-1,n_2-1}(\min(f, 1/f))-F_{n_1-1,n_2-1}(\max(f,1/f))+1
  \label{eq-tests-cv-pv}
  \ee
   \label{theo-tests-cvt}
  \end{shadethm}


\begin{exnn}{Network Monitoring Again} We want to test whether
the data in groups 1 and 2 in \exref{ex-fruitflies} have the
same variance. We have $\eta =   0.3494, \xi =    2.862$; the
$F$ statistic is $0.8306$ so we accept $H_0$, i.e. that the
variance is the same. Alternatively, we can use
\eref{eq-tests-cv-pv} and find $p=0.7239$ , which is large so
we accept $H_0$.

Of course, we are more interested in comparing the 3 groups
together. We apply \eref{lrs-pv-32} and find as likelihood
ratio statistic $\ell=0.0862$. The asymptotic approximation
gives $p\approx 0.9174$, but since the number of samples $n$ is
not large we do not trust it. We evaluate
\eref{eq-tests-cvdsmc} by Monte Carlo simulation; with $R=10^4$
replicates we find a confidence interval for the estimated
$p$-value of $[ 0.9247; 0.9347]$. We conclude that the
$p$-value is very large so we accept that the variance is the
same.
\end{exnn}

\section{Asymptotic Results}
\mylabel{sec-lrs-asymptotic} In many cases it is hard to find
the exact distribution of a test statistic. An interesting
feature of likelihood ratio tests is that we have a simple
asymptotic result. We used this result already in the test for
equal variance in \sref{sec-test-chi2}.
\subsection{Likelihood Ratio Statistic}
The
following theorem derives immediately from \thref{theo-mle2}.
\begin{shadethm}\cite{davison2003sm}
Consider a likelihood ratio test (\sref{sec-mletests}) with
$\Theta=\Theta_1\times \Theta_2$, where $\Theta_1, \Theta_2$
are open subsets of $\Reals^{q_1}, \Reals^{q_2}$ and denote
$\theta=(\theta_1,\theta_2)$. Consider the likelihood ratio
test of $H_0: \theta_2=0$ against $H_1:\theta_2 \neq 0$. Assume
that the conditions in \dref{def-mle} hold. Then,
approximately, for large sample sizes, under $H_0$, $2 lrs\sim
\chi^2_{q_2}$, where $lrs$ is the likelihood ratio statistic.

It follows that the $p$-value of the likelihood ratio test can
be approximated for large sample sizes by \be p^* \approx
1-\chi^2_{q_2}\left( 2lrs\right) \ee \label{theo-mle-t} where
$q_2$ is the number of degrees of freedom that $H_1$ adds to
$H_0$.\end{shadethm}
%\mq{stats-qopi8}{Under $H_0$, what is the distribution of $l_X(\hat{\theta}) -
%l_X(\hat{\theta}_0)$ in \dref{def-lrs1}~?}{$\frac{1}{2} \chi^2_{p}$, where $p$ is the difference
%between the dimension of $\Theta$ and $\Theta_0$.}



\begin{exnn}{Application to \exref{ex-vojex} (Compiler Options)}
Using \thref{theo-anova} and \thref{theo-mle-t} we find that
$$
2 lrs\eqdef N \ln\left(1+\frac{SS2}{SS1}\right)\sim \chi^2_1
$$

The corresponding $p$-values are:\\

{\footnotesize
 \pro{Parameter Set 1 \input{npdChi270}}\\
 \pro{Parameter Set 2 \input{npdChi245}}\\
 \pro{Parameter Set 3 \input{npdChi210}}\\
}

They are all very close to the exact values (given by ANOVA in
\tref{tab-test-anova1}).
\end{exnn}

%\subsection{Application to Non Paired Data, Different Variances}
%We show in this section how the asymptotic result may be useful
%when the hypothesis of same variance in ANOVA does not hold. We
%show the method on a simple example; it applies by analogy to
%the general ANOVA cases. Assume we are given two unpaired
%series of data, and we want to test whether they have the same
%mean. The model is
% \be X_i = \mu_1 + \epsilon_{1,i}\;\; Y_j = \mu_2 + \epsilon_{2,j}
% \ee
% with $\epsilon_{i,j}\sim$ iid $N_{0,\sigma^2}$.
%We apply \sref{sec-mletests} and compute first the MLEs. For
%the unrestricted maximum likelihood estimator
%$\hat{\theta}=(\hat{\mu}_1, \hat{\mu}_2,
% \hat{\sigma}_1,\hat{\sigma}_2)$ we find
%\begin{itemize}
%  \item $\hat{\mu}_1=\bar{x}$, $\hat{\mu}_2=\bar{y}$,
%  \item $\hat{\sigma}_1^2=\frac{1}{m}S_{xx}$, $\hat{\sigma}_2^2=\frac{1}{n}S_{yy}$,
%  \item $l_{x,y}(\hat{\theta})=cst -\frac{m}{2}\ln S_{xx} -\frac{n}{2}\ln S_{yy}$
%\end{itemize}
%(with $S_{xx}=\sum_i (x_i-\bar{x})^2$). The restricted maximum
%likelihood estimator $\hat{\theta}_0=(\hat{\mu}, \hat{\mu},
% \hat{\sigma}_1',\hat{\sigma}_2')$ cannot be obtained explicitly. We have
%
%\begin{equation}\mylabel{eq-test1}
%  l_{x,y}(\mu,\mu,\sigma_1',\sigma_2') =
%  cst - \frac{m}{2} \ln  \sigma_1'^2 -\frac{S_{xx}+m(\bar{x}-\mu)^2}{2
%  \sigma_1'^2} - \frac{n}{2} \ln  \sigma_2'^2 -\frac{S_{yy}+n(\bar{y}-\mu)^2}{2
%  \sigma_2'^2}
%\end{equation}
% By differentiating with respect to $\mu$ we find that
%\begin{equation}\mylabel{eq-test2}
%  \hat{\mu}=\frac{S_1 \bar{x} + S_2\bar{y}}{S_1+S_2}
%\end{equation}
%with $S_1=m/{\sigma}_1'^2$ and $S_2=n/{\sigma}_2'^2$. By
%substituting \eref{eq-test2} in \eref{eq-test1}, we obtain the
%log-likelihood as a function of two variables
%${\sigma}_1',{\sigma}_2'$. We maximize it numerically to obtain
%$\hat{\sigma}_1',\hat{\sigma}_2'$.
%
%The likelihood ratio statistic is
%\begin{equation}\mylabel{eq-test3}
%  T=\frac{1}{2}\left(l_{x,y}(\hat{\theta}) - l_{x,y}(\hat{\theta}_0)
%\right)
%\end{equation}
%%$$
%%=
%%2\frac{(\bar{X}-\bar{Y})^2}{\frac{\hat{\sigma}_1'^2}{m}+\frac{\hat{\sigma}_2'^2}{n}}
%%-\frac{m}{2}\ln\frac{S_{XX}}{\hat{\sigma}_1'^2}+\frac{S_{XX}}{2\hat{\sigma}_1'^2}
%%-\frac{n}{2}\ln\frac{S_{YY}}{\hat{\sigma}_2'^2}+\frac{S_{YY}}{2\hat{\sigma}_2'^2}
%%$$
%it can be computed once we know all values of MLEs. We know that,
%under the assumption that the means are equal, and asymptotically,
%$T \sim \chi^2_p$. Now $p=1$ since there are $4$ free parameters
%for $\theta$ and $3$ for $\theta_0$. Thus, the test for equality
%of means has rejection region of the form $C = \{ T > \eta \}$
%where $\chi^2_1(\eta)=1-\alpha$, i.e. $\eta=\xi^2$ where $\xi$ is
%the $1-\alpha/2$ quantile of the standard normal distribution.
%
%\mq{mle-anova-qii8}{What is the $p$-value of the test~?} {
%$$
%p^*=1 - \chi^2_1(T)= 2(1-N(\sqrt{T}))
%$$ where $N$ is the standard normal distribution function. We reject equality of means when $p^*$
%is smaller than $\alpha$.}
%
%\begin{ex}{Network Monitoring Again}
%Assume now that we would have the same data, except for the
%number of samples per group which would be equal to $800$. Let
%us repeat the analysis for the comparison of groups 2 and 3.
%The test for same variance gives the same value $F =  0.8306$
%but now $\eta =   0.8704, \xi = 1.149$, so we reject the
%hypothesis of same variance. We cannot apply ANOVA. However we
%can still test for equal mean, using the general asymptotics
%presented above, since the number of samples is large.
%
%Zum Gl\"{u}ck, $n$ is large, so we apply the maximum likelihood
%estimator asymptotics instead explained above. We find
%\begin{itemize}
%  \item Unrestricted model: $\hat{\mu}_1=63.56, \hat{\mu}_2=56.76,\;\hat{\sigma}_1=16.43574, \;\hat{\sigma}_2=14.91346$
%  \item Restricted model: $\hat{\mu}=59.8145, \;\hat{\sigma}_1'=16.8571, \;\hat{\sigma}_2'=15.2230$
%  \item $T=45.9 > 3.84$ thus we reject the hypothesis of equalities of means (at a size $\alpha=0.05$).
%\end{itemize}
%\end{ex}

\subsection{Pearson Chi-squared Statistic and Goodness of Fit}
\mylabel{sec-chi-gof} We can apply the large sample asymptotic to
goodness of fit tests as defined in \sref{sec-simple-gof}. This
gives a simpler way to compute the $p$-value, and allows to extend
the test to the \nt{composite goodness of fit} test, defined as
follows.

\paragraph{Composite Goodness of Fit}
Similar to \sref{sec-simple-gof}, assume we are given $n$ data
points $x_1, ...,x_n$, generated from an iid sequence, and we
want to verify whether their common distribution comes from a
given family of distributions $F(|\theta)$ where the parameter
$\theta$ is in some set $\Theta_0$. We say that the test is
composite because the null hypothesis has several possible
values of $\theta$. We compare the empirical histograms: we
partition the set of values of ${\vX}$ into \nt{bins} $B_i$,
$i=1...I$. Let $N_i=\sum_{k=1}^n \ind{B_i}(X_k)$ (number of
observation that fall in bin $B_i$) and $q_i=\P_\theta\{X_1\in
B_i\}$. If the data comes from a distribution $F(|\theta)$ the
distribution of $N_i$ is multinomial $M_{n,\vec{q}(\theta)}$.
The likelihood ratio statistic test is
\begin{quote}
$H_0$: $N_i$ comes from a multinomial distribution
$M_{n,\vec{q}(\theta)}$, with $\theta \in \Theta_0$

against

$H_1$: $N_i$ comes from a multinomial distribution $M_{n,\vec{p}}$
for some arbitrary $\vec{p}$. \end{quote}

We now compute the likelihood ratio statistic. The maximum
likelihood estimator of the parameter under $H_1$ is the same
as in \sref{sec-simple-gof}. Let $\hat{\theta}$ be the maximum
likelihood estimator of $\theta$ under $H_0$. The likelihood
ratio statistic is thus
 \be \mylabel{eq-tests-lrsm2}
 lrs = \sum_{i=1}^k n_i
 \ln\frac{n_i}{n q_i(\hat{\theta})}
 \ee
The $p$-value is
 \be \sup_{\theta \in \Theta_0}
 \P\left(
\sum_{i=1}^k N_i
 \ln\frac{N_i}{n q_i} >\sum_{i=1}^k n_i
 \ln\frac{n_i}{n q_i(\hat{\theta})}
 \right)
 \ee
 where $\vec{N}$ has the multinomial distribution
 $M_{n,\vec{q}(\hat{\theta})}$. It can be computed by Monte Carlo
 simulation as in the case of a simple test, but this may be
 difficult because of the supremum.

 An alternative for large $n$ is to use the asymptotic result in
 \thref{theo-mle-t}. It says that, for large $n$, under $H_0$, the distribution
 of $2 lrs  $  is approximately $\chi^2_{q_2}$, with $q_2=$ the number of degrees of freedom that $H_1$ adds to $H_0$.
 Here $H_0$ has $k_0$ degrees of freedom (where $k_0$ is the dimension of $\Theta_0$) and
 $H_1$ has $I-1$ degrees of freedom (where $I$ is the number of bins).
 Thus the $p$-value of the
 test is approximately
 \be
 1- \chi^2_{I-k_0-1}( 2 lrs)
 \ee
%where $\chi^2_{I-k_0-1}$ is the cdf of the chi-square
%distribution with $I-k_0-1$ degrees of freedom.

\begin{exnn}{Impact of estimation of $(\mu, \sigma)$}
We want to test whether the data set on the right of \fref{tests-ft} has a normal distribution.
We use a histogram with 10 bins.
We need first to estimate $\hat{\theta}=(\hat{\mu},\hat{\sigma})$.

1. Assume we do this by fitting a line to the qqplot. We obtain $\hat{\mu}=-0.2652,\hat{\sigma}     =    0.8709$.
The values of $n q_i(\hat{\theta})$ and $n_i$ are:
{\footnotesize
\begin{verbatim}
   7.9297    7.0000
   11.4034    9.0000
   18.0564   17.0000
   21.4172   21.0000
   19.0305   14.0000
   12.6672   17.0000
    6.3156    6.0000
    2.3583    4.0000
    0.6594    3.0000
    0.1624    2.0000
\end{verbatim}}
The likelihood ratio statistic as in \eref{eq-tests-lrsm2} is
$lrs =7.6352$. The $p$-value is obtained using a $\chi^2_7$
distribution ($q_2=10-2-1$): $p1 =    0.0327$, thus we would
reject normality at size $0.05$.

2. It might not be good to simply fit $(\mu, \sigma)$ on the
qqplot. A better way is to use estimation theory, which
suggests to find $(\mu, \sigma)$ that maximizes the log
likelihood of the model. This is equivalent to minimizing the
likelihood ratio statistic $l_{H_1}({\vx})-l_{\mu,
\sigma}({\vx})$ (note that the value of $l_{H_1}({\vx})$ is
easy to compute). We do this with a numerical optimization
procedure and find now $\hat{\mu}=-0.0725,\hat{\sigma} =
1.0269$. The corresponding values of $n q_i(\hat{\theta})$ and
$n_i$ are now: {\footnotesize
\begin{verbatim}
    8.3309    7.0000
    9.5028    9.0000
   14.4317   17.0000
   17.7801   21.0000
   17.7709   14.0000
   14.4093   17.0000
    9.4783    6.0000
    5.0577    4.0000
    2.1892    3.0000
    1.0491    2.0000
\end{verbatim}}
Note how the true value of $\hat{\mu},\hat{\sigma}$ provides a
better fit to the tail of the histogram. The likelihood ratio
statistic  is now  $lrs =2.5973$, which also shows a much
better fit. The $p$-value, obtained using a $\chi^2_7$
distribution is now $p1 =    0.6362$, thus we accept that the
data is normal.

3. Assume we would ignore that $(\mu, \sigma)$ is estimated
from the data, but would do as if the test were a simple
goodness of fit test, with $H_0:$~``The distribution is
$N_{-0.0725,1.0269}$" instead of $H_0:$~``The distribution is
normal". We would compute the $p$-value using a $\chi^2_9$
distribution ($q_2=10-1$) and would obtain: $p2 =   0.8170$, a
value larger than the true $p$-value. This is quite general: if
we estimate some parameter and pretend it is a priori known,
then we overestimate the $p$-value.
\end{exnn}
\paragraph{Pearson Chi-Squared Statistic. }
In the case where $n$ is large, $2 \times$ the likelihood ratio
statistic can be replaced by the \nt{Pearson chi-squared statistic},
which has the same asymptotic distribution. It is defined by
 \be pcs =
\sum_{i=1}^I \frac{(n_i - n q_i(\hat{\theta}))^2}{n
q_i(\hat{\theta})}\ee


Indeed, when $n$ is large we expect, under $H_0$ that $n_i - n
q_i(\hat{\theta})$ is relatively small, i.e. $
\epsilon_i=\frac{n_i}{n q_i(\hat{\theta})}-1
$
 is small. An approximation of $2 lrs$ is
 found from the second order development around $\epsilon =0$:
$ \ln (1+\epsilon)=\epsilon -  \frac{1}{2}\epsilon^2 +
o(\epsilon^2)
 $
 and thus
 \bearn
 lrs &=& \sum_i n_i \frac{n_i}{n q_i(\hat{\theta})}
 n\sum_i (1 + \epsilon_i) q_i(\hat{\theta}) \ln(1 + \epsilon_i)\\
  & = &
  n \sum_i \left(\epsilon_i-\frac{1}{2}\epsilon_i^2 + o(\epsilon_i^2)(1+\epsilon_i)q_i(\hat{\theta})
  \right)\\
& = & n \sum_i
q_i(\hat{\theta})\epsilon_i\left(1-\frac{1}{2}\epsilon_i +
o(\epsilon_i)(1+\epsilon_i)\right)\\
&=& n \sum_i
q_i(\hat{\theta})\epsilon_i\left(1+\frac{1}{2}\epsilon_i +
o(\epsilon_i)\right)\\
&=&n \sum_i q_i(\hat{\theta})\epsilon_i+ n \sum_i
q_i(\hat{\theta})\frac{1}{2}\epsilon_i^2 + n\sum_i o(\epsilon_i^2)
 \eearn
 Note that $\sum_i q_i(\hat{\theta})\epsilon_i=0$ thus
 \be
 lrs \approx \frac{1}{2} pcs
 \ee

 The Pearson Chi-squared statistic was historically developed before
 the theory of likelihood ratio tests, which explains why it is
 commonly used.

 In summary, for large $n$, the composite goodness of fit test is solved by computing either
 $2 lrs$ or $pcs$. The $p$-value is $1- \chi^2_{n-k_0-1}( 2 lrs)$ or
 $1- \chi^2_{I-k_0-1}( pcs)$. If either is small, we reject $H_0$,
 i.e. we reject that the distribution of $X_i$ comes from the family
 of distributions $F(|\theta)$.

\paragraph{Simple Goodness of Fit Test.} This is a special case of the composite test.
In this case $q_2=I-1$ and thus the $p$-value of the test
(given in \eref{eq-test-pv} can be approximated for large $n$
by  $1- \chi^2_{I-1}( 2 lrs)$ or $\chi^2_{I-1}( pcs)$. Also,
the likelihood ratio statistic $\sum_{i=1}^k n_i
 \ln\frac{n_i}{n q_i}$ can be replaced by the
Pearson-Chi-Squared statistic, equal to
 \be
\sum_{i=1}^I \frac{(n_i - n q_i)^2}{n q_i}
 \ee
\begin{exnn}{Mendel's peas, continuation of
\exref{ex-tests-mendel}} The likelihood ratio statistic is
$lrs=0.3092$ and we found by Monte Carlo a $p$-value $p^* =
0.9191\pm 0.0458$. By the asymptotic result, we can approximate the
$p$-value by $\chi^2_3(2 lrs)= 0.8922$.

The Pearson Chi-squared statistic is $pcs=0.6043$, very close
to $2 lrs=0.618$. The corresponding $p$-value is $0.8954$.
\end{exnn}

\subsection{Test of Independence}
The same ideas as in \sref{sec-chi-gof} can be applied to a
\nt{test of independence}. We are given a sequence $(x_k,
y_k)$, which we interpret as a sample of the sequence $(X_k,
Y_k)$, $k=1,...,n$. The sequence is iid ($(X_k, Y_k)$ is
independent of $(X_{k'}, Y_{k'})$ for $k\neq k'$ and both have the same
distribution). We are interested in knowing whether $X_k$ is
independent of $Y_k$.

To this end, we compute an empirical histogram of $(X, Y)$, as follows. We
partition the set of values
of $X$ [resp. $Y$] into $I$ [resp. $J$] bins $B_i$ [resp. $C_j$].
Let $N_{i,j}=\sum_{k=1}^n \ind{B_i}(X_k) \ind{C_j}(Y_k)$
(number of observation that fall in bin $(B_i, C_j)$) and $p_{i,j}=\P\{X_1\in
B_i \mand Y_1\in
C_j \}$. The distribution of
$N$ is multinomial. The test of independence is
\begin{quote}
 $H_0$:~``$p_{i,j}=q_i r_j$ for some $q$ and $r$ such that $\sum_i q_i = \sum_j r_j =1$"

 against

$H_1$:~``$p_{i,j}$ is arbitrary"
\end{quote}

The maximum likelihood estimator under $H_0$ is
$\hat{p}^0_{i,j}=\frac{n_{i.}}{n}\frac{n_{.j}}{n}$ where
$n_{i,j}=\sum_{k=1}^n \ind{B_i}(x_k) \ind{C_j}(y_k)$ and
 \be
 \bracket{
 n_{i.}= \sum_j n_{i,j}\\
 n_{.j}= \sum_i n_{i,j}
 }
 \ee
The maximum likelihood estimator under $H_1$ is
$\hat{p}^1_{i,j}=\frac{n_{i,j}}{n}$. The likelihood ratio
statistic is thus
 \be
 lrs = \sum_{i,j} n_{i,j} \ln \frac{n n_{i,j}}{n_{i.} n_{.j}}
 \ee
To compute the $p$-value, we use, for large $n$, a
$\chi^2_{q_2}$ distribution. The numbers of degrees of freedom
under $H_1$ is $IJ-1$, under $H_0$ it is $(I-1)+(J-1)$, thus
$q_2=(IJ-1)-(I-1)-(J-1)=(I-1)(J-1)$. The $p$-value is thus
 \be p^*= \left(1-\chi^2_{(I-1)(J-1)}\right)(2 lrs)
 \ee
As in \sref{sec-chi-gof}, $2 lrs$ can be replaced, for large $n$, by the Pearson
Chi-squared
statistic:
 \be
 pcs = \sum_{i,j} \frac{\left(n_{i,j}-\frac{n_{i.}n_{.j}}{n}\right)^2}{\frac{n_{i.}n_{.j}}{n}}
 \ee

 \begin{ex}{Brassica Oleracea Gemmifera} A survey was conducted at the campus cafeteria, where customers were asked whether
 they like Brussels sprouts. The answers are:
 \begin{center}
 \begin{tabular}{|c|c c c c||c c|}
 \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   $i\backslash j$  & \multicolumn{2}{c}{Male}& \multicolumn{2}{c||}{Female} &\multicolumn{2}{c|}{\emph{Total}} \\ \hline
   \input{bru} %\\
%   Likes & 1 & 2 & 12 \\
%   Dislikes & 3 & 4 & 34 \\
%   No Answer / Neutral & 5 & 6 & 56 \\ \hline \hline
%   \emph{Total} & 135 & 246 & 123456
 \hline
 \end{tabular}
 \end{center} We would like to test whether affinity to Brussels sprouts
 is independent of customer's gender.
%
 Here we have $I=3$ and $J=2$, so we use a $\chi^2$ distribution with $q_2=2$
 degrees of freedom. The likelihood ratio statistic and the $p$-value are
  \input{bru-lrs}
so we accept $H_0$, i.e. affinity to Brussels sprouts is independent of gender.
%
Note that the Pearson Chi-squared statistic is
  \input{bru-pcs}
  which is very close to $2 lrs$. \end{ex}
%\mq{testmle-qkl8}{In Example 9.3, what is the $\chi^2$ statistic
%$X_e$ obtained if we do not do the approximation in the last two
%lines of Eq (49)~? Compare to the value $25.01$. Under $H_0$, is
%this new statistic exactly or asymptotically $\chi^2$~?}{Take your
%favorite computer tool and find:
%%
%\newcommand{\statsWeberEx}[2]{#1 \ln( #1 / #2 )}
%$$
%  \begin{array}{rll}
%X_e &= 2 (&
% \statsWeberEx{370}{337.171}+ \\
% &&\statsWeberEx{300}{332.829}+ \\
% &&\statsWeberEx{950}{982.829}+ \\
% &&\statsWeberEx{1003}{970.171}
% ) \\& = 8.6552
%  \end{array}$$
%which is not far from the approximate value $8.642$. Under $H_0$,
%$X_e$ is asymptotically $\chi^2_1$.}
\section{Other Tests}
\subsection{Goodness of Fit Tests based on Ad-Hoc Pivots}\mylabel{sec-gof}
\label{sec-skku}  In addition to the Pearson $\chi^2$ test, the
following two tests are often used. They apply to a continuous
distribution, thus do not require quantizing the observations.
Assume $X_i$, $i=1,..,n$ are iid samples. We want to test
$H_0$: the distribution of $X_i$ is $F$ against non $H_0$.

Define the empirical distribution $\hat{F}$ by
\begin{equation}\mylabel{eq-empiri-1}
  \hat{F}(x)\eqdef\frac{1}{n}\sum_{i=1}^n 1_{\{X_i \leq x\}}
\end{equation}


\paragraph{\nt{Kolmogorov-Smirnov}}
A \nt{pivot} is a function of the data whose probability distribution under $H_0$ is the same for all $\theta \in\Theta_0$. For this test the pivot is
$$
T=\sup_x|\hat{F}(x)-F(x)|
$$
That the distribution of this random variable is independent of $F$ is not entirely obvious, but
can be derived easily in the case where $F$ is continuous and strictly increasing, as follows.
The idea is to change the scale on the $x$-axis by $u=F(x)$. Formally, define
$$
U_i=F(X_i)
$$
so that $U_i\sim U(0,1)$. Also
$$\hat{F}(x)=\frac{1}{n}\sum_i 1_{\{X_i \leq x\}}=\frac{1}{n}\sum_i 1_{\{U_i \leq
F(x)\}}=\hat{G}(F(x))$$ where $\hat{G}$ is the empirical distribution of the sample $U_i$,
$i=1,...,n$. By the change of variable $u=F(x)$, it comes
 $$T=\sup_{u\in[0,1]} |\hat{G}(u)-u|
 $$
which shows that the distribution of $T$ is independent of
$F$. Its distribution is tabulated in statistical software
packages. For a large $n$, its tail can be approximated by
$\tau \approx \sqrt{-(\ln \alpha)/2}$ where
$\P(T>\tau)=\alpha$.

\paragraph{\nt{Anderson-Darling}}
Here the pivot is
$$
 A =n \int_{\Reals}\frac{\left(\hat{F}(x)-F(x)\right)^2}{F(x)(1-F(x))}dF(x)
$$
The test is similar to K-S but is less sensitive to outliers.
\mq{q-dskdsklsdfkl}{Show that $A$ is indeed a pivot.}{ Use the
fact that $\hat{F}(x)=\hat{G}(F(x))$ and do the change of
variable $u=F(x)$ in the integral.}


\begin{ex}{File Transfer Data}
We would like to test whether the data in \fref{tests-ft} and its
log are normal. We cannot directly apply Kolmogorov Smirnov since
we do not know exactly in advance the parameters of the normal
distribution to be tested against. An approximate method is to
estimate the slope and intercept of the straight line in the qqplot. We obtain {\footnotesize
 \begin{verbatim}
 Original Data
 slope     =    0.8155
 intercept =    1.0421

 Transformed Data
 slope     =    0.8709
 intercept =   -0.2652
 \end{verbatim}
}For example, this means that for the original data we take for
$H_0$: ``the distribution is $N(\mu=1.0421, \sigma^2=0.8155^2)$".
We can now use the Kolmogorov-Smirnov test and obtain
{\footnotesize
 \begin{verbatim}
 Original Data
 h =     1           p =    0.0493

 Transformed Data
 h =     0           p =    0.2415
 \end{verbatim}
}

Thus the test rejects the normality assumption for the original
data and accepts it for the transformed data.

This way of doing is approximate in that we used estimated
parameters for $H_0$. This introduces some bias, similar to using
the normal statistic instead of student when we have a normal
sample. The bias should be small when the data sample is large,
which is the case here.

A fix to this problem is to use a variant of KS, for example
the Lilliefors test, or to use different normality tests such
as Jarque Bera (see \exref{ex-stats-n2}) or
Shapiro-Wilk\nfs{Shapiro-Wilk to be done}. The Lilliefors test
is a heuristic that corrects the $p$-value of the KS to account
for the uncertainty due to estimation. In this specific
example, with the Lilliefors test we obtain the same results.
\mylabel{ex-stats-n1}
\end{ex}
\begin{figure}
\center \Ifig{testsFt}{0.7}{0.3}%
 \nfs{fileTransfer.dat normalTests.m}
 \mycaption{Normal qqplots of file transfer data and its logarithm.}
 \mylabel{tests-ft}
\end{figure}

\paragraph{Jarque-Bera.}
 The \nt{Jarque-Bera} statistic is used to test whether an iid sample comes from a normal
 distribution. It uses the skewness and kurtosis indices $\gamma_1$ and $\gamma_2$ defined in \sref{sec-skew-kurt}.
 The test statistic is equal to $\frac{n}{6}\left(\hat{\gamma}_1^2 +
 \frac{\hat{\gamma}_2^2}{4}\right)$, the distribution of which is asymptotically $\chi^2_2$ for
 large sample size $n$. In the formula, $\hat{\gamma}_1$ and $\hat{\gamma}_2$ are the sample
 indices of skewness and kurtosis, obtained by replacing expectations by sample averages in
 \eref{eq-defkappa}.

 \begin{ex}{Application to \exref{ex-stats-n1}}
 We would like to test whether the data in \exref{ex-stats-n1} and its
transform are normal. {\footnotesize
 \begin{verbatim}
 Original Data          h = 1          p =    0.0010
 Transformed Data       h = 0          p =    0.1913
 \end{verbatim}
 }

The conclusions are the same as in
\exref{ex-stats-n1}, but for the original data
the normality assumption is clearly rejected,
whereas it was borderline in \exref{ex-stats-n1}.
\mylabel{ex-stats-n2}
\end{ex}


\subsection{Robust Tests}
\mylabel{sec-test-robust} We give two examples of test that make
no assumption on the distribution of the sample (but assume it is
iid). They are \nt{non parametric} in the sense that they do not
assume a parameterized family of densities.


\paragraph{Median Test}
The model is $X_i \sim$ iid with some distribution $F()$ with a
density. We want to test
\begin{quote}
$H_0$:~``the median of $F$ is $0$" against $H_1$:~``unspecified"
\end{quote}
A simple test is based on confidence interval, as mentioned in
\sref{sec-tajt}. Let $I({\vx})$ be a confidence interval for
the median (\thref{theo-conf-ci-median}). We reject $H_0$ if
 \be
 0 \nin I({\vx})
 \ee
This test is robust in the sense that it makes no assumption other
than independence.

\paragraph{Wilcoxon Signed Rank Test.} It is used for testing
equality of distribution in paired experiments. It tests
 \begin{quote}
 $H_0$: $X_1,...X_n$ is iid with a common symmetric, continuous
 distribution, the median
 of which is 0

 against

 $H_1$: $X_1,...X_n$ is iid with a common symmetric, continuous
 distribution
 \end{quote}
The \nt{Wilcoxon Signed Rank} Statistic is
$$
W=\sum_{j=1}^n \mbox{rank}(|X_j|) \mbox{sign}(X_j)
$$
where $\mbox{rank}(|X_j|)$ is the rank in increasing order
(the smallest value has rank 1) and $\mbox{sign}(X_j)$ is $-1$
for negative data, $+1$ for positive, and $0$ for null data.
If the median is positive, then many values with high rank
will be positive and $W$ will tend to be positive and large.
We reject the null hypothesis when $|W|$ is large.

It can be shown that the distribution of $W$ under $H_0$ is
always the same. It is tabulated and contained in software
packages. For non small data samples, it can easily be
approximated by a normal distribution. The mean and variance
under can easily be computed:
$$\E_{H_0}(W)=\sum_{j=1}^n \E_{H_0}(\mbox{rank}(|X_j|)\E_{H_0}(\mbox{sign}(X_j))$$ since under $H_0$
$\mbox{rank}(|X_j|)$ is independent of $\mbox{sign}(X_j)$.
Thus $E_{H_0}(W)=0$. The variance is
$$ \E_{H_0}(W^2)=\sum_{j=1}^n \E_{H_0}(\mbox{rank}(|X_j|)^2\mbox{sign}(X_j)^2)=
\sum_{j=1}^n \E_{H_0}(\mbox{rank}(|X_j|)^2)
$$since $\mbox{sign}(X_j)^2=1$. Now $\sum_j
\mbox{rank}(|X_j|)^2=\sum_j j^2$ is non random thus
$$\var_{H_0}(W)=\sum_{j=1}^n \E_{H_0}(\mbox{rank}(|X_j|)^2)=\E_{H_0}(\sum_j \mbox{rank}(|X_j|)^2
)=\sum_{j=1}^n j^2=\frac{n(n+1)(2n+1)}{6}$$

For large $n$, the test at size $\alpha$ rejects $H_0$ if
$\abs{W}
> \eta \sqrt{\frac{n(n+1)(2n+1)}{6}}$ with
$N_{0,1}(\eta)=1-\frac{\alpha}{2}$ (e.g. $\eta=1.96$ at size
$0.05$). The $p$-value is:
 \be
 p = 2 \lp 1 - N_{0,1}\lp
 \frac{\abs{W}}{\sqrt{\frac{n(n+1)(2n+1)}{6}} } \rp \rp
 \ee



\begin{exnn}{Paired Data}This is a variant of \exref{ex-test-paired}. Consider again
the reduction in run time due to a new compiler option, as given
in \fref{fig-conf-c2} on \pgref{fig-conf-c2}. We want to test
whether the reduction is significant. We assume the data is iid,
but not necessarily normal. The median test gives a confidence
interval
 \ben I({\vx})= [2.9127  ; 33.7597] \een
 which does not contain $0$ so we reject $H_0$.

 Alternatively, let us use the Wilcoxon Signed Rank test. We obtain the $p$-value
 \ben
p =   2.3103e-005 \een and thus this test also rejects $H_0$.
\end{exnn}

\paragraph{Wilcoxon Rank Sum Test and Kruskal-Wallis.}
The \nt{Wilcoxon Rank Sum} Test is used for testing equality of
distribution in non paired experiments. It tests
 \begin{quote}
  $H_0$: the two samples come from the same continuous distribution

against

$H_1$: the distributions of the two samples are continuous and
differ by a location shift
 \end{quote}

Let $X^1_i$, $i=1...n_1$ and $X^2_i$, $i=1...n_2$ be the two iid
sequences that the data is assumed to be a sample of. The
\nt{Wilcoxon Rank Sum Statistic} $R$ is the sum of the ranks of
the first sample in the concatenated sample.

As for the Wilcoxon signed rank test, its distribution under
the null hypothesis depends only on the sample sizes and can be
tabulated or, for a large sample size, approximated by a normal
distribution. The mean and variance under $H_0$ are
 \be
m_{n_1, n_2}=\frac{n_1(n_1+n_2+1)}{2} \ee
 \be
v_{n_1, n_2}=\frac{n_1 n_2(n_1+n_2+1)}{12} \ee

We reject $H_0$ when the rank sum statistic deviates largely
from its expectation under $H_0$. For large $n_1$ and $n_2$,
the $p$-value is
 \be
 p = 2 \lp 1 - N_{0,1}\lp
 \frac{\abs{R-m_{n_1, n_2}}}{\sqrt{v_{n_1, n_2}} } \rp \rp
 \ee

\begin{ex}{Non Paired Data}
\mylabel{ex-npd-wil}The Wilcoxon rank sum test applied to
\exref{ex-vojex} gives the following $p$-values:

{\footnotesize
 \pro{Parameter Set 1 \input{npdWil70}}\\
 \pro{Parameter Set 2 \input{npdWil45}}\\
 \pro{Parameter Set 3 \input{npdWil10}}\\
}

The results are the same as with ANOVA. $H_0$ (same distribution)
is accepted for the 3rd data set only, at size$=0.05$.
\end{ex}

The \nt{Kruskal-Wallis} test is a generalization of Wilcoxon Rank
Sum to more than 2 non paired data series. It tests ($H_0$): the
samples come from the same distribution against ($H_1$): the
distributions may differ by a location shift.

\subsubsection{Turning Point Test}\label{sec-turning-point}
This is a test of iid-ness. It tests
 \begin{quote}
 $H_0$: $X_1, ..., X_n$ is iid

 against

 $H_1$: $X_1, ..., X_n$ is not iid
 \end{quote}

We say that the vector $X_1, ...,X_n$ is monotonic at index $i$
($i \in \{2,...,n-1\}$) if
 \begin{quote}$X_{i-1} \leq X_i \leq
X_{i+ 1}$ or $X_{i-1} \geq X_i \geq X_{i+ 1}$ \end{quote} and
we say that there is a \nt{turning point} at $i$ if the vector
$X_1, ...,X_n$ is not monotonic at $i$. Under $H_0$, the
probability of a turning point at $i$ is $2/3$ (to see why,
list all possible cases for the relative orderings of $X_{i-1},
X_i, X_{i+ 1}$).

More precisely, let $T$ be the number of turning points in
$X_1, ..., X_n$. It can be shown
\cite{brockwell2002introduction,weber-ts} that, for large $n$,
$T$ is approximately $N_{\frac{2n-4}{3}, \frac{16n-29}{90}}$.
Thus the $p$-value is, approximatively for large $n$:
 \be
 p = 2 \lp 1 - N_{0,1}\lp
 \frac{\abs{T-\frac{2n-4}{3}} }{\sqrt{\frac{16n-29}{90}}}  \rp \rp
 \ee
%
%\subsubsection{Difference Sign} Also a test of iid-ness
%  $S$ is the number of couples $(s,t)$
%   with $s<t$ such that $X_i < X_j$. Under
%  $H_0$, and for
%  large $n$, $S\sim N(\mu_n, s^2_n)$ with $\mu_n=n(n-1)/4$ and $s^2_n=n(n-1)(2n+5)/72$. If $|S-\mu_n|$ is
%  large, we reject $H_0$. Further, if $S$ is large, the sign of $S$ is an indication of the size
%  of a trend.

\section{Proofs}
\begin{petit}
 \subsection*{Proof of \thref{theo-tests-cvt}}
We make a likelihood ratio test and compute the likelihood
ratio statistic. We need first to compute the maximum
likelihood under $H_1$. The log-likelihood of the model is
 \be
 l_{\vx}(\vec{\mu},\vec{\sigma})=
 -\frac{1}{2}\left[ \ln(2 \pi)+\sum_{i=1}^I \left(2 n_i
   \ln(\sigma_i)+\sum_{j=1}^{n_i} \frac{({\vx}_{i,j}-\mu_i)^2}{\sigma_i^2}
   \right)\right]
   \label{eq-fis-23}
 \ee
 To find the maximum under $H_1$, observe that the terms in the summation do not have cross
dependencies, thus we can maximize each of the $I$ terms
separately. The maximum of the $i$th term is for
$\mu_i=\hat{\mu}_i$ and $\sigma_i^2=s_i^2$,
 and thus
 \be
l_{\vx}(H_1)= -\frac{1}{2}\left[ \ln(2 \pi)+\sum_{i=1}^I
n_i\left(
   2\ln(s_i)+1
   \right)\right]= -\frac{1}{2}\left[ \ln(2 \pi)+n+2\sum_{i=1}^I n_i
   \ln(s_i)\right]
 \ee
Under $H_0$ the likelihood is as in \eref{eq-fis-23} but with
$\sigma_i$ replaced by the common value $\sigma$. To find the
maximum,we use the ANOVA
 theorem \ref{mle-theo-de-base}. The maximum is for
 $\mu_i=\hat{\mu}_i$ and $\sigma^2=s^2$
 and thus
\be l_{\vx}(H_0)=  -\frac{1}{2}\left[ \ln(2 \pi)+\sum_{i=1}^I
n_i \frac{s^2_i}{s^2}+2n
   \ln(s)\right]= -\frac{1}{2}\left[ \ln(2 \pi)+n+2n
   \ln(s)\right]
 \ee
 The test statistic is the likelihood ratio statistic $\ell=l_{\vx}(H_1)-l_{\vx}(H_0)$:
 and thus \be
 2 \ell = n \ln(s^2) - \sum_{i=1}^I n_i \ln(s^2_i)
 \ee
The test has the form: reject $H_0$ when $lrs > K$ for some
constant $K$. The $p$-value can be obtained using Monte-Carlo
simulation.
 The problem is now to compute $\P(T>2 \ell)$ where $T$ is a random
 variable distributed like
 \be n \ln(s^2) - \sum_{i=1}^I n_i
 \ln(s_i^2)  \label{lrs-pv-32s}\ee
 and assuming $H_0$ holds. Observe that all we need is to generate the random variables
 $s_i^2$. They are independent, and $Z_i= n_i s_i$ is distributed like
 $\sigma^2 \chi^2_{n_i-1}$ (\coref{cor-norm-de-base}). Note that $T$ is independent of the
 specific value of the unknown but fixed parameter $\sigma$, thus we
 can let $\sigma=1$ in the Monte Carlo simulation, which proves
 \eref{eq-tests-cvdsmc}.
Alternatively, one can use the large sample asymptotic in
\thref{theo-mle-t}, which gives \eref{eq-tests-dfjsjdsf}.

When $I=2$ we can rewrite the likelihood ratio statistic as
 \be
 \ell= \frac{1}{2}\left[n \ln(n_1 F + n_2) - n_1 \ln(F)\right] + C
 \ee
where  $C$ is a constant term (assuming $n_1$ and $n_2$ are
fixed) and $
 F=\frac{s_1^2}{s_2^2}
$. The derivative of $\ell$ with respect to $F$ is
 \be
 \frac{\partial \ell}{\partial F}=\frac{n_1 n_2 (F-1)}{2F (n_1 F +n_2)}
 \ee
 thus $\ell$ decreases with $F$ for $F<1$ and increases for $F>1$. Thus
 the rejection region, defined as $\{\ell > K\}$, is also of the form
 $\{F< K_1 \mor F> K_2\}$.
 Now define
 \be
 f= \frac{\hat{\sigma}_1^2}{\hat{\sigma}_2^2}
 \ee Note that $f = F C'$
 where $C'$ is a constant, so the set $\{ F<K_1 \mor F > K_2\}$ is equal to
 the set $\{ f \eta \mor f >\xi\}$ with $\eta=C' K_1$
 and $\xi=C' K_2$.
Under $H_0$, the distribution of $F$ is Fisher with parameters
$(n_1-1,n_2-1)$ (\thref{mle-theo-de-base}), so we have a Fisher
test. The bounds $\eta$ and $\xi$ are classically computed by
the conditions
 $F_{n_1-1,n_2-1}(\eta)=\alpha/2$,
 $F_{n_1-1,n_2-1}(\xi)=1-\alpha/2$.

 Last, note that by the properties of the Fisher distribution, the particular
 choice of $\eta$ and $\xi$ above is such that $\xi=1/\eta$, so the
rejection region is also defined by $\lc f>\xi \mor f<1/\xi
\rc$, which is the same as $\lc \max(f,1/f)>\xi\rc$, a form
suitable to define a $p$-value (\sref{sec-tests-pv}). Let
$g=\max(f, 1/f)$ and $X \sim F_{m,n}$, then
 \bearn p &\eqdef& P(\max(X,
1/X)> g)=P(X < 1/g)+P(X > g)%
 =F_{n_1-1,n_2-1}(1/g)+ 1-F_{n_1-1,n_2-1}(g) \eearn which,
together with $1/g=\min(f,1/f)$, shows \eref{eq-tests-cv-pv}.
\end{petit}

\section{Review}
\subsection{Tests Are Just Tests}
\begin{enumerate}
  \item The first test to do on any data is a visual exploration. In most cases, this is
  sufficient.
  \item Testing for a $0$ mean or $0$ median is the same as
  computing a confidence interval for the mean or the median.
  \item Tests work only if the underlying assumptions are
      verified (in particular, practically all tests, even
      robust ones, assume the data comes from an iid
 sample).
  \item Some tests work under a larger spectrum of assumptions (for example: even if the data is
  not normal). They are called robust tests. They should be
  preferred whenever possible.
  \item
  Test whether the same variance assumption holds, otherwise, use robust tests or asymptotic results.
  \item If you perform a large number of different tests on the same data, then the probability
  of rejecting $H_0$ is larger than for any single test. So, contrary to non-statistical tests, increasing the
  number of tests does not always improve the decision.
\end{enumerate}
\subsection{Review Questions}
\mq{tests-q100a}
 {What is the critical region of a test ?
 }
 { Call ${\vx}$ the data used for the test. The critical region $C$ is a set
 of possible values of ${\vx}$ such
 that when ${\vx}\in C$ we
 reject $H_0$.
 }

 \mq{tests-q101}
 {What is a type 1 error ? A type 2 error ? The size of a
 test ?
 }
 {A type 1 error occurs when the test says ``do not accept $H_0$" whereas the truth is $H_0$.
 A type 2 error occurs when the test says ``accept $H_0$" whereas the truth is $H_1$. The size
 of a test is $\sup_{\theta \mst H_0 \mbox{ is true}}\P_{\theta}(C)$ ( = the worst case probability of a
 type 1 error).
 }
%\mq{tests-q200}
% {If a test says ``do not accept $H_0$", can we conclude that $H_1$ is true ?
% }
% {No, consider \exref{ex-test-paired}. The first test says ``do not accept $\mu=0$",
%  from
% which we cannot conclude that $H_1 = \{``\mu=40"\}$ is true, since the second test says
% ``do not accept
% $\mu=40$". A test can only gives an indication that some hypothesis is wrong.
% }
 \mq{tests-q-834}{What is the $p$-value of a test~?}{It applies to tests where the critical region is of the form
 $T({\vx}) > m$ where $T({\vx})$ is the test statistic and ${\vx}$ is the data. The $p$-value is
 the probability that $T({\vX}) > T({\vx})$ where ${\vX}$ is a hypothetical data set, generated under the
 hypothesis $H_0$. We reject $H_0$ at size $\alpha$ if $p>\alpha$.}
 \mq{tests-q-837}{What are the hypotheses for ANOVA~?}{The data is iid, gaussian, with perhaps different means but with same variance.}
 \mq{tests-q-847}{How do you compute a $p$-value by Monte Carlo simulation~?}{Generate $R$ iid samples $T^r$ from
 the distribution of $T({\vX})$ under $H_0$ and compute $\hat{p}$ as the fraction of times that
 $T^r>T({\vx})$. We need $R$ large enough (typically order of 10000) and compute a confidence interval for $\hat{p}$ using
 \thref{theo-est-proba}.}
 \mq{tests-q-347}{A Monte Carlo simulation returns $\hat{p}=0$ as estimate of the $p$-value. Can we reject $H_0$?}{We need to know
 the number $R$ of Monte Carlo replicates. A confidence interval for $p$ is $[0; 3.869/R]$ at level 95\%; if $R$
 is order of 100
 or more, we can reject $H_0$ at size $0.05$.}
 \mq{tests-q-747}
{What is a likelihood ratio statistic test in a nest model ?
What can we say in general  about its $p$-value~?}{The test
statistic is $lrs$, the
 log of the likelihood ratios under $H_1$ and $H_0$, and the test rejects $H_0$ if $lrs$ is large. The nested model means that the model is parametric, with some
 sets $\Theta_0 \subset \Theta$ such that $H_0$ means $\theta \in \Theta_0$ and $H_1$ means $\theta \in \Theta \setminus \Theta_0$.
 If the
 data sample is large, the $p$-value is obtained by saying that, under $H_0$,  $2 lrs\sim \chi_{q_2}$, where $q_2$ is
 the number of degrees of freedom that $H_1$ adds to $H_0$.}
%
%\section{Exercises}
%\paragraph{Useful S-PLUS commands} {\tt aov, tables.model, summary, wilcox.test}
%\paragraph{Useful Matlab commands} {\tt anova1, anova2,
%anovan}: (analysis of variance, differ only in the format of the
%data model); \pro{ttest}: student test; {\tt ranksum, signrank}:
%Wilcoxon non-parametric tests.
% m{p-3101}
% m{p-3201}
%m{p-3001}
 %\m{p-35001-Homework-confInterval}
